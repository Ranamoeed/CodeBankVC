{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ranamoeed/CodeBankVC/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import time\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from coqpit import Coqpit\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "from TTS.tts.utils.visual import plot_spectrogram\n",
        "from TTS.utils.audio import AudioProcessor\n",
        "from TTS.utils.audio.numpy_transforms import mulaw_decode\n",
        "from TTS.utils.io import load_fsspec\n",
        "from TTS.vocoder.datasets.wavernn_dataset import WaveRNNDataset\n",
        "from TTS.vocoder.layers.losses import WaveRNNLoss\n",
        "from TTS.vocoder.models.base_vocoder import BaseVocoder\n",
        "from TTS.vocoder.utils.distribution import sample_from_discretized_mix_logistic, sample_from_gaussian\n",
        "\n",
        "def stream(string, variables):\n",
        "    sys.stdout.write(f\"\\r{string}\" % variables)\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(dims, dims, kernel_size=1, bias=False)\n",
        "        self.conv2 = nn.Conv1d(dims, dims, kernel_size=1, bias=False)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(dims)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(dims)\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.batch_norm2(x)\n",
        "        return x + residual\n",
        "\n",
        "class MelResNet(nn.Module):\n",
        "    def __init__(self, num_res_blocks, in_dims, compute_dims, res_out_dims, pad):\n",
        "        super().__init__()\n",
        "        k_size = pad * 2 + 1\n",
        "        self.conv_in = nn.Conv1d(in_dims, compute_dims, kernel_size=k_size, bias=False)\n",
        "        self.batch_norm = nn.BatchNorm1d(compute_dims)\n",
        "        self.layers = nn.ModuleList()\n",
        "        for _ in range(num_res_blocks):\n",
        "            self.layers.append(ResBlock(compute_dims))\n",
        "        self.conv_out = nn.Conv1d(compute_dims, res_out_dims, kernel_size=1)\n",
        "    def forward(self, x):\n",
        "        x = self.conv_in(x)\n",
        "        x = self.batch_norm(x)\n",
        "        x = F.relu(x)\n",
        "        for f in self.layers:\n",
        "            x = f(x)\n",
        "        x = self.conv_out(x)\n",
        "        return x\n",
        "\n",
        "class Stretch2d(nn.Module):\n",
        "    def __init__(self, x_scale, y_scale):\n",
        "        super().__init__()\n",
        "        self.x_scale = x_scale\n",
        "        self.y_scale = y_scale\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.size()\n",
        "        x = x.unsqueeze(-1).unsqueeze(3)\n",
        "        x = x.repeat(1, 1, 1, self.y_scale, 1, self.x_scale)\n",
        "        return x.view(b, c, h * self.y_scale, w * self.x_scale)\n",
        "\n",
        "class UpsampleNetwork(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        feat_dims,\n",
        "        upsample_scales,\n",
        "        compute_dims,\n",
        "        num_res_blocks,\n",
        "        res_out_dims,\n",
        "        pad,\n",
        "        use_aux_net,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.total_scale = np.cumproduct(upsample_scales)[-1]\n",
        "        self.indent = pad * self.total_scale\n",
        "        self.use_aux_net = use_aux_net\n",
        "        if use_aux_net:\n",
        "            self.resnet = MelResNet(num_res_blocks, feat_dims, compute_dims, res_out_dims, pad)\n",
        "            self.resnet_stretch = Stretch2d(self.total_scale, 1)\n",
        "        self.up_layers = nn.ModuleList()\n",
        "        for scale in upsample_scales:\n",
        "            k_size = (1, scale * 2 + 1)\n",
        "            padding = (0, scale)\n",
        "            stretch = Stretch2d(scale, 1)\n",
        "            conv = nn.Conv2d(1, 1, kernel_size=k_size, padding=padding, bias=False)\n",
        "            conv.weight.data.fill_(1.0 / k_size[1])\n",
        "            self.up_layers.append(stretch)\n",
        "            self.up_layers.append(conv)\n",
        "    def forward(self, m):\n",
        "        if self.use_aux_net:\n",
        "            aux = self.resnet(m).unsqueeze(1)\n",
        "            aux = self.resnet_stretch(aux)\n",
        "            aux = aux.squeeze(1)\n",
        "            aux = aux.transpose(1, 2)\n",
        "        else:\n",
        "            aux = None\n",
        "        m = m.unsqueeze(1)\n",
        "        for f in self.up_layers:\n",
        "            m = f(m)\n",
        "        m = m.squeeze(1)[:, :, self.indent : -self.indent]\n",
        "        return m.transpose(1, 2), aux\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, scale, pad, num_res_blocks, feat_dims, compute_dims, res_out_dims, use_aux_net):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "        self.pad = pad\n",
        "        self.indent = pad * scale\n",
        "        self.use_aux_net = use_aux_net\n",
        "        self.resnet = MelResNet(num_res_blocks, feat_dims, compute_dims, res_out_dims, pad)\n",
        "    def forward(self, m):\n",
        "        if self.use_aux_net:\n",
        "            aux = self.resnet(m)\n",
        "            aux = torch.nn.functional.interpolate(aux, scale_factor=self.scale, mode=\"linear\", align_corners=True)\n",
        "            aux = aux.transpose(1, 2)\n",
        "        else:\n",
        "            aux = None\n",
        "        m = torch.nn.functional.interpolate(m, scale_factor=self.scale, mode=\"linear\", align_corners=True)\n",
        "        m = m[:, :, self.indent : -self.indent]\n",
        "        m = m * 0.045\n",
        "        return m.transpose(1, 2), aux\n",
        "\n",
        "@dataclass\n",
        "class WavernnArgs(Coqpit):\n",
        "    rnn_dims: int = 512\n",
        "    fc_dims: int = 512\n",
        "    compute_dims: int = 128\n",
        "    res_out_dims: int = 128\n",
        "    num_res_blocks: int = 10\n",
        "    use_aux_net: bool = True\n",
        "    use_upsample_net: bool = True\n",
        "    upsample_factors: List[int] = field(default_factory=lambda: [4, 8, 8])\n",
        "    mode: str = \"mold\"\n",
        "    mulaw: bool = True\n",
        "    pad: int = 2\n",
        "    feat_dims: int = 80\n",
        "\n",
        "class Wavernn(BaseVocoder):\n",
        "    def __init__(self, config: Coqpit):\n",
        "        super().__init__(config)\n",
        "        if isinstance(self.args.mode, int):\n",
        "            self.n_classes = 2**self.args.mode\n",
        "        elif self.args.mode == \"mold\":\n",
        "            self.n_classes = 3 * 10\n",
        "        elif self.args.mode == \"gauss\":\n",
        "            self.n_classes = 2\n",
        "        else:\n",
        "            raise RuntimeError(\"Unknown model mode value - \", self.args.mode)\n",
        "        self.ap = AudioProcessor(**config.audio.to_dict())\n",
        "        self.aux_dims = self.args.res_out_dims // 4\n",
        "        if self.args.use_upsample_net:\n",
        "            assert (\n",
        "                np.cumproduct(self.args.upsample_factors)[-1] == config.audio.hop_length\n",
        "            ), \" [!] upsample scales needs to be equal to hop_length\"\n",
        "            self.upsample = UpsampleNetwork(\n",
        "                self.args.feat_dims,\n",
        "                self.args.upsample_factors,\n",
        "                self.args.compute_dims,\n",
        "                self.args.num_res_blocks,\n",
        "                self.args.res_out_dims,\n",
        "                self.args.pad,\n",
        "                self.args.use_aux_net,\n",
        "            )\n",
        "        else:\n",
        "            self.upsample = Upsample(\n",
        "                config.audio.hop_length,\n",
        "                self.args.pad,\n",
        "                self.args.num_res_blocks,\n",
        "                self.args.feat_dims,\n",
        "                self.args.compute_dims,\n",
        "                self.args.res_out_dims,\n",
        "                self.args.use_aux_net,\n",
        "            )\n",
        "        if self.args.use_aux_net:\n",
        "            self.I = nn.Linear(self.args.feat_dims + self.aux_dims + 1, self.args.rnn_dims)\n",
        "            self.rnn1 = nn.GRU(self.args.rnn_dims, self.args.rnn_dims, batch_first=True)\n",
        "            self.rnn2 = nn.GRU(self.args.rnn_dims + self.aux_dims, self.args.rnn_dims, batch_first=True)\n",
        "            self.fc1 = nn.Linear(self.args.rnn_dims + self.aux_dims, self.args.fc_dims)\n",
        "            self.fc2 = nn.Linear(self.args.fc_dims + self.aux_dims, self.args.fc_dims)\n",
        "            self.fc3 = nn.Linear(self.args.fc_dims, self.n_classes)\n",
        "        else:\n",
        "            self.I = nn.Linear(self.args.feat_dims + 1, self.args.rnn_dims)\n",
        "            self.rnn1 = nn.GRU(self.args.rnn_dims, self.args.rnn_dims, batch_first=True)\n",
        "            self.rnn2 = nn.GRU(self.args.rnn_dims, self.args.rnn_dims, batch_first=True)\n",
        "            self.fc1 = nn.Linear(self.args.rnn_dims, self.args.fc_dims)\n",
        "            self.fc2 = nn.Linear(self.args.fc_dims, self.args.fc_dims)\n",
        "            self.fc3 = nn.Linear(self.args.fc_dims, self.n_classes)\n",
        "    def forward(self, x, mels):\n",
        "        bsize = x.size(0)\n",
        "        h1 = torch.zeros(1, bsize, self.args.rnn_dims).to(x.device)\n",
        "        h2 = torch.zeros(1, bsize, self.args.rnn_dims).to(x.device)\n",
        "        mels, aux = self.upsample(mels)\n",
        "        if self.args.use_aux_net:\n",
        "            aux_idx = [self.aux_dims * i for i in range(5)]\n",
        "            a1 = aux[:, :, aux_idx[0] : aux_idx[1]]\n",
        "            a2 = aux[:, :, aux_idx[1] : aux_idx[2]]\n",
        "            a3 = aux[:, :, aux_idx[2] : aux_idx[3]]\n",
        "            a4 = aux[:, :, aux_idx[3] : aux_idx[4]]\n",
        "        x = (\n",
        "            torch.cat([x.unsqueeze(-1), mels, a1], dim=2)\n",
        "            if self.args.use_aux_net\n",
        "            else torch.cat([x.unsqueeze(-1), mels], dim=2)\n",
        "        )\n",
        "        x = self.I(x)\n",
        "        res = x\n",
        "        self.rnn1.flatten_parameters()\n",
        "        x, _ = self.rnn1(x, h1)\n",
        "        x = x + res\n",
        "        res = x\n",
        "        x = torch.cat([x, a2], dim=2) if self.args.use_aux_net else x\n",
        "        self.rnn2.flatten_parameters()\n",
        "        x, _ = self.rnn2(x, h2)\n",
        "        x = x + res\n",
        "        x = torch.cat([x, a3], dim=2) if self.args.use_aux_net else x\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = torch.cat([x, a4], dim=2) if self.args.use_aux_net else x\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "    def inference(self, mels, batched=None, target=None, overlap=None):\n",
        "        self.eval()\n",
        "        output = []\n",
        "        start = time.time()\n",
        "        rnn1 = self.get_gru_cell(self.rnn1)\n",
        "        rnn2 = self.get_gru_cell(self.rnn2)\n",
        "        with torch.no_grad():\n",
        "            if isinstance(mels, np.ndarray):\n",
        "                mels = torch.FloatTensor(mels).to(str(next(self.parameters()).device))\n",
        "            if mels.ndim == 2:\n",
        "                mels = mels.unsqueeze(0)\n",
        "            wave_len = (mels.size(-1) - 1) * self.config.audio.hop_length\n",
        "            mels = self.pad_tensor(mels.transpose(1, 2), pad=self.args.pad, side=\"both\")\n",
        "            mels, aux = self.upsample(mels.transpose(1, 2))\n",
        "            if batched:\n",
        "                mels = self.fold_with_overlap(mels, target, overlap)\n",
        "                if aux is not None:\n",
        "                    aux = self.fold_with_overlap(aux, target, overlap)\n",
        "            b_size, seq_len, _ = mels.size()\n",
        "            h1 = torch.zeros(b_size, self.args.rnn_dims).type_as(mels)\n",
        "            h2 = torch.zeros(b_size, self.args.rnn_dims).type_as(mels)\n",
        "            x = torch.zeros(b_size, 1).type_as(mels)\n",
        "            if self.args.use_aux_net:\n",
        "                d = self.aux_dims\n",
        "                aux_split = [aux[:, :, d * i : d * (i + 1)] for i in range(4)]\n",
        "            for i in range(seq_len):\n",
        "                m_t = mels[:, i, :]\n",
        "                if self.args.use_aux_net:\n",
        "                    a1_t, a2_t, a3_t, a4_t = (a[:, i, :] for a in aux_split)\n",
        "                x = torch.cat([x, m_t, a1_t], dim=1) if self.args.use_aux_net else torch.cat([x, m_t], dim=1)\n",
        "                x = self.I(x)\n",
        "                h1 = rnn1(x, h1)\n",
        "                x = x + h1\n",
        "                inp = torch.cat([x, a2_t], dim=1) if self.args.use_aux_net else x\n",
        "                h2 = rnn2(inp, h2)\n",
        "                x = x + h2\n",
        "                x = torch.cat([x, a3_t], dim=1) if self.args.use_aux_net else x\n",
        "                x = F.relu(self.fc1(x))\n",
        "                x = torch.cat([x, a4_t], dim=1) if self.args.use_aux_net else x\n",
        "                x = F.relu(self.fc2(x))\n",
        "                logits = self.fc3(x)\n",
        "                if self.args.mode == \"mold\":\n",
        "                    sample = sample_from_discretized_mix_logistic(logits.unsqueeze(0).transpose(1, 2))\n",
        "                    output.append(sample.view(-1))\n",
        "                    x = sample.transpose(0, 1).type_as(mels)\n",
        "                elif self.args.mode == \"gauss\":\n",
        "                    sample = sample_from_gaussian(logits.unsqueeze(0).transpose(1, 2))\n",
        "                    output.append(sample.view(-1))\n",
        "                    x = sample.transpose(0, 1).type_as(mels)\n",
        "                elif isinstance(self.args.mode, int):\n",
        "                    posterior = F.softmax(logits, dim=1)\n",
        "                    distrib = torch.distributions.Categorical(posterior)\n",
        "                    sample = 2 * distrib.sample().float() / (self.n_classes - 1.0) - 1.0\n",
        "                    output.append(sample)\n",
        "                    x = sample.unsqueeze(-1)\n",
        "                else:\n",
        "                    raise RuntimeError(\"Unknown model mode value - \", self.args.mode)\n",
        "                if i % 100 == 0:\n",
        "                    self.gen_display(i, seq_len, b_size, start)\n",
        "        output = torch.stack(output).transpose(0, 1)\n",
        "        output = output.cpu()\n",
        "        if batched:\n",
        "            output = output.numpy()\n",
        "            output = output.astype(np.float64)\n",
        "            output = self.xfade_and_unfold(output, target, overlap)\n",
        "        else:\n",
        "            output = output[0]\n",
        "        if self.args.mulaw and isinstance(self.args.mode, int):\n",
        "            output = mulaw_decode(wav=output, mulaw_qc=self.args.mode)\n",
        "        fade_out = np.linspace(1, 0, 20 * self.config.audio.hop_length)\n",
        "        output = output[:wave_len]\n",
        "        if wave_len > len(fade_out):\n",
        "            output[-20 * self.config.audio.hop_length :] *= fade_out\n",
        "        self.train()\n",
        "        return output\n",
        "\n",
        "    def gen_display(self, i, seq_len, b_size, start):\n",
        "        gen_rate = (i + 1) / (time.time() - start) * b_size / 1000\n",
        "        realtime_ratio = gen_rate * 1000 / self.config.audio.sample_rate\n",
        "        stream(\n",
        "            \"%i/%i -- batch_size: %i -- gen_rate: %.1f kHz -- x_realtime: %.1f  \",\n",
        "            (i * b_size, seq_len * b_size, b_size, gen_rate, realtime_ratio),\n",
        "        )\n",
        "\n",
        "    def fold_with_overlap(self, x, target, overlap):\n",
        "        _, total_len, features = x.size()\n",
        "        num_folds = (total_len - overlap) // (target + overlap)\n",
        "        extended_len = num_folds * (overlap + target) + overlap\n",
        "        remaining = total_len - extended_len\n",
        "        if remaining != 0:\n",
        "            num_folds += 1\n",
        "            padding = target + 2 * overlap - remaining\n",
        "            x = self.pad_tensor(x, padding, side=\"after\")\n",
        "        folded = torch.zeros(num_folds, target + 2 * overlap, features).to(x.device)\n",
        "        for i in range(num_folds):\n",
        "            start = i * (target + overlap)\n",
        "            end = start + target + 2 * overlap\n",
        "            folded[i] = x[:, start:end, :]\n",
        "        return folded\n",
        "\n",
        "    @staticmethod\n",
        "    def get_gru_cell(gru):\n",
        "        gru_cell = nn.GRUCell(gru.input_size, gru.hidden_size)\n",
        "        gru_cell.weight_hh.data = gru.weight_hh_l0.data\n",
        "        gru_cell.weight_ih.data = gru.weight_ih_l0.data\n",
        "        gru_cell.bias_hh.data = gru.bias_hh_l0.data\n",
        "        gru_cell.bias_ih.data = gru.bias_ih_l0.data\n",
        "        return gru_cell\n",
        "\n",
        "    @staticmethod\n",
        "    def pad_tensor(x, pad, side=\"both\"):\n",
        "        b, t, c = x.size()\n",
        "        total = t + 2 * pad if side == \"both\" else t + pad\n",
        "        padded = torch.zeros(b, total, c).to(x.device)\n",
        "        if side in (\"before\", \"both\"):\n",
        "            padded[:, pad : pad + t, :] = x\n",
        "        elif side == \"after\":\n",
        "            padded[:, :t, :] = x\n",
        "        return padded\n",
        "\n",
        "    @staticmethod\n",
        "    def xfade_and_unfold(y, target, overlap):\n",
        "        num_folds, length = y.shape\n",
        "        target = length - 2 * overlap\n",
        "        total_len = num_folds * (target + overlap) + overlap\n",
        "        silence_len = overlap // 2\n",
        "        fade_len = overlap - silence_len\n",
        "        silence = np.zeros((silence_len), dtype=np.float64)\n",
        "        t = np.linspace(-1, 1, fade_len, dtype=np.float64)\n",
        "        fade_in = np.sqrt(0.5 * (1 + t))\n",
        "        fade_out = np.sqrt(0.5 * (1 - t))\n",
        "        fade_in = np.concatenate([silence, fade_in])\n",
        "        fade_out = np.concatenate([fade_out, silence])\n",
        "        y[:, :overlap] *= fade_in\n",
        "        y[:, -overlap:] *= fade_out\n",
        "        unfolded = np.zeros((total_len), dtype=np.float64)\n",
        "        for i in range(num_folds):\n",
        "            start = i * (target + overlap)\n",
        "            end = start + target + 2 * overlap\n",
        "            unfolded[start:end] += y[i]\n",
        "        return unfolded\n",
        "\n",
        "    def load_checkpoint(\n",
        "        self, config, checkpoint_path, eval=False, cache=False\n",
        "    ):\n",
        "        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"), cache=cache)\n",
        "        self.load_state_dict(state[\"model\"])\n",
        "        if eval:\n",
        "            self.eval()\n",
        "            assert not self.training\n",
        "        def train_step(self, batch: Dict, criterion: Dict) -> Tuple[Dict, Dict]:\n",
        "        mels = batch[\"input\"]\n",
        "        waveform = batch[\"waveform\"]\n",
        "        waveform_coarse = batch[\"waveform_coarse\"]\n",
        "\n",
        "        y_hat = self.forward(waveform, mels)\n",
        "        if isinstance(self.args.mode, int):\n",
        "            y_hat = y_hat.transpose(1, 2).unsqueeze(-1)\n",
        "        else:\n",
        "            waveform_coarse = waveform_coarse.float()\n",
        "        waveform_coarse = waveform_coarse.unsqueeze(-1)\n",
        "        loss_dict = criterion(y_hat, waveform_coarse)\n",
        "        return {\"model_output\": y_hat}, loss_dict\n",
        "\n",
        "    def eval_step(self, batch: Dict, criterion: Dict) -> Tuple[Dict, Dict]:\n",
        "        return self.train_step(batch, criterion)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def test(\n",
        "        self, assets: Dict, test_loader: \"DataLoader\", output: Dict\n",
        "    ) -> Tuple[Dict, Dict]:\n",
        "        ap = self.ap\n",
        "        figures = {}\n",
        "        audios = {}\n",
        "        samples = test_loader.dataset.load_test_samples(1)\n",
        "        for idx, sample in enumerate(samples):\n",
        "            x = torch.FloatTensor(sample[0])\n",
        "            x = x.to(next(self.parameters()).device)\n",
        "            y_hat = self.inference(x, self.config.batched, self.config.target_samples, self.config.overlap_samples)\n",
        "            x_hat = ap.melspectrogram(y_hat)\n",
        "            figures.update(\n",
        "                {\n",
        "                    f\"test_{idx}/ground_truth\": plot_spectrogram(x.T),\n",
        "                    f\"test_{idx}/prediction\": plot_spectrogram(x_hat.T),\n",
        "                }\n",
        "            )\n",
        "            audios.update({f\"test_{idx}/audio\": y_hat})\n",
        "        return figures, audios\n",
        "\n",
        "    def test_log(\n",
        "        self, outputs: Dict, logger: \"Logger\", assets: Dict, steps: int\n",
        "    ) -> Tuple[Dict, np.ndarray]:\n",
        "        figures, audios = outputs\n",
        "        logger.eval_figures(steps, figures)\n",
        "        logger.eval_audios(steps, audios, self.ap.sample_rate)\n",
        "\n",
        "    @staticmethod\n",
        "    def format_batch(batch: Dict) -> Dict:\n",
        "        waveform = batch[0]\n",
        "        mels = batch[1]\n",
        "        waveform_coarse = batch[2]\n",
        "        return {\"input\": mels, \"waveform\": waveform, \"waveform_coarse\": waveform_coarse}\n",
        "\n",
        "    def get_data_loader(\n",
        "        self,\n",
        "        config: Coqpit,\n",
        "        assets: Dict,\n",
        "        is_eval: True,\n",
        "        samples: List,\n",
        "        verbose: bool,\n",
        "        num_gpus: int,\n",
        "    ):\n",
        "        ap = self.ap\n",
        "        dataset = WaveRNNDataset(\n",
        "            ap=ap,\n",
        "            items=samples,\n",
        "            seq_len=config.seq_len,\n",
        "            hop_len=ap.hop_length,\n",
        "            pad=config.model_args.pad,\n",
        "            mode=config.model_args.mode,\n",
        "            mulaw=config.model_args.mulaw,\n",
        "            is_training=not is_eval,\n",
        "            verbose=verbose,\n",
        "        )\n",
        "        sampler = DistributedSampler(dataset, shuffle=True) if num_gpus > 1 else None\n",
        "        loader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=1 if is_eval else config.batch_size,\n",
        "            shuffle=num_gpus == 0,\n",
        "            collate_fn=dataset.collate,\n",
        "            sampler=sampler,\n",
        "            num_workers=config.num_eval_loader_workers if is_eval else config.num_loader_workers,\n",
        "            pin_memory=True,\n",
        "        )\n",
        "        return loader\n",
        "\n",
        "    def get_criterion(self):\n",
        "        return WaveRNNLoss(self.args.mode)\n",
        "\n",
        "    @staticmethod\n",
        "    def init_from_config(config: \"WavernnConfig\"):\n",
        "        return Wavernn(config)"
      ],
      "metadata": {
        "id": "H5sIdSch9WgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from inspect import signature\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from coqpit import Coqpit\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from trainer.trainer_utils import get_optimizer, get_scheduler\n",
        "\n",
        "from TTS.utils.audio import AudioProcessor\n",
        "from TTS.utils.io import load_fsspec\n",
        "from TTS.vocoder.datasets.gan_dataset import GANDataset\n",
        "from TTS.vocoder.layers.losses import DiscriminatorLoss, GeneratorLoss\n",
        "from TTS.vocoder.models import setup_discriminator, setup_generator\n",
        "from TTS.vocoder.models.base_vocoder import BaseVocoder\n",
        "from TTS.vocoder.utils.generic_utils import plot_results\n",
        "\n",
        "\n",
        "class GAN(BaseVocoder):\n",
        "    def __init__(self, config: Coqpit, ap: AudioProcessor = None):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "        self.model_g = setup_generator(config)\n",
        "        self.model_d = setup_discriminator(config)\n",
        "        self.train_disc = False\n",
        "        self.y_hat_g = None\n",
        "        self.ap = ap\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model_g.forward(x)\n",
        "\n",
        "    def inference(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model_g.inference(x)\n",
        "\n",
        "    def train_step(self, batch: Dict, criterion: Dict, optimizer_idx: int) -> Tuple[Dict, Dict]:\n",
        "        outputs = {}\n",
        "        loss_dict = {}\n",
        "        x = batch[\"input\"]\n",
        "        y = batch[\"waveform\"]\n",
        "        if optimizer_idx not in [0, 1]:\n",
        "            raise ValueError(\" [!] Unexpected `optimizer_idx`.\")\n",
        "        if optimizer_idx == 0:\n",
        "            y_hat = self.model_g(x)[:, :, : y.size(2)]\n",
        "            self.y_hat_g = y_hat\n",
        "            self.y_hat_sub = None\n",
        "            self.y_sub_g = None\n",
        "            if y_hat.shape[1] > 1:\n",
        "                self.y_hat_sub = y_hat\n",
        "                y_hat = self.model_g.pqmf_synthesis(y_hat)\n",
        "                self.y_hat_g = y_hat\n",
        "                self.y_sub_g = self.model_g.pqmf_analysis(y)\n",
        "            scores_fake, feats_fake, feats_real = None, None, None\n",
        "            if self.train_disc:\n",
        "                if self.config.diff_samples_for_G_and_D:\n",
        "                    x_d = batch[\"input_disc\"]\n",
        "                    y_d = batch[\"waveform_disc\"]\n",
        "                    with torch.no_grad():\n",
        "                        y_hat = self.model_g(x_d)\n",
        "                    if y_hat.shape[1] > 1:\n",
        "                        y_hat = self.model_g.pqmf_synthesis(y_hat)\n",
        "                else:\n",
        "                    x_d = x.clone()\n",
        "                    y_d = y.clone()\n",
        "                    y_hat = self.y_hat_g\n",
        "                if len(signature(self.model_d.forward).parameters) == 2:\n",
        "                    D_out_fake = self.model_d(y_hat.detach().clone(), x_d)\n",
        "                    D_out_real = self.model_d(y_d, x_d)\n",
        "                else:\n",
        "                    D_out_fake = self.model_d(y_hat.detach())\n",
        "                    D_out_real = self.model_d(y_d)\n",
        "                if isinstance(D_out_fake, tuple):\n",
        "                    scores_fake, feats_fake = D_out_fake\n",
        "                    if D_out_real is None:\n",
        "                        scores_real, feats_real = None, None\n",
        "                    else:\n",
        "                        scores_real, feats_real = D_out_real\n",
        "                else:\n",
        "                    scores_fake = D_out_fake\n",
        "                    scores_real = D_out_real\n",
        "                loss_dict = criterion[optimizer_idx](scores_fake, scores_real)\n",
        "                outputs = {\"model_outputs\": y_hat}\n",
        "        if optimizer_idx == 1:\n",
        "            scores_fake, feats_fake, feats_real = None, None, None\n",
        "            if self.train_disc:\n",
        "                if len(signature(self.model_d.forward).parameters) == 2:\n",
        "                    D_out_fake = self.model_d(self.y_hat_g, x)\n",
        "                else:\n",
        "                    D_out_fake = self.model_d(self.y_hat_g)\n",
        "                D_out_real = None\n",
        "                if self.config.use_feat_match_loss:\n",
        "                    with torch.no_grad():\n",
        "                        D_out_real = self.model_d(y)\n",
        "                if isinstance(D_out_fake, tuple):\n",
        "                    scores_fake, feats_fake = D_out_fake\n",
        "                    if D_out_real is None:\n",
        "                        feats_real = None\n",
        "                    else:\n",
        "                        _, feats_real = D_out_real\n",
        "                else:\n",
        "                    scores_fake = D_out_fake\n",
        "                    feats_fake, feats_real = None, None\n",
        "            loss_dict = criterion[optimizer_idx](\n",
        "                self.y_hat_g, y, scores_fake, feats_fake, feats_real, self.y_hat_sub, self.y_sub_g\n",
        "            )\n",
        "            outputs = {\"model_outputs\": self.y_hat_g}\n",
        "        return outputs, loss_dict\n",
        "\n",
        "    def _log(self, name: str, ap: AudioProcessor, batch: Dict, outputs: Dict) -> Tuple[Dict, Dict]:\n",
        "        y_hat = outputs[0][\"model_outputs\"] if self.train_disc else outputs[1][\"model_outputs\"]\n",
        "        y = batch[\"waveform\"]\n",
        "        figures = plot_results(y_hat, y, ap, name)\n",
        "        sample_voice = y_hat[0].squeeze(0).detach().cpu().numpy()\n",
        "        audios = {f\"{name}/audio\": sample_voice}\n",
        "        return figures, audios\n",
        "\n",
        "    def train_log(\n",
        "        self, batch: Dict, outputs: Dict, logger: \"Logger\", assets: Dict, steps: int\n",
        "    ) -> Tuple[Dict, np.ndarray]:\n",
        "        figures, audios = self._log(\"eval\", self.ap, batch, outputs)\n",
        "        logger.eval_figures(steps, figures)\n",
        "        logger.eval_audios(steps, audios, self.ap.sample_rate)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def eval_step(self, batch: Dict, criterion: nn.Module, optimizer_idx: int) -> Tuple[Dict, Dict]:\n",
        "        self.train_disc = True\n",
        "        return self.train_step(batch, criterion, optimizer_idx)\n",
        "\n",
        "    def eval_log(\n",
        "        self, batch: Dict, outputs: Dict, logger: \"Logger\", assets: Dict, steps: int\n",
        "    ) -> Tuple[Dict, np.ndarray]:\n",
        "        figures, audios = self._log(\"eval\", self.ap, batch, outputs)\n",
        "        logger.eval_figures(steps, figures)\n",
        "        logger.eval_audios(steps, audios, self.ap.sample_rate)\n",
        "\n",
        "    def load_checkpoint(\n",
        "        self,\n",
        "        config: Coqpit,\n",
        "        checkpoint_path: str,\n",
        "        eval: bool = False,\n",
        "        cache: bool = False,\n",
        "    ) -> None:\n",
        "        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"), cache=cache)\n",
        "        if \"model_disc\" in state:\n",
        "            self.model_g.load_checkpoint(config, checkpoint_path, eval)\n",
        "        else:\n",
        "            self.load_state_dict(state[\"model\"])\n",
        "            if eval:\n",
        "                self.model_d = None\n",
        "                if hasattr(self.model_g, \"remove_weight_norm\"):\n",
        "                    self.model_g.remove_weight_norm()\n",
        "\n",
        "    def on_train_step_start(self, trainer) -> None:\n",
        "        self.train_disc = trainer.total_steps_done >= self.config.steps_to_start_discriminator\n",
        "\n",
        "    def get_optimizer(self) -> List:\n",
        "        optimizer1 = get_optimizer(\n",
        "            self.config.optimizer, self.config.optimizer_params, self.config.lr_gen, self.model_g\n",
        "        )\n",
        "        optimizer2 = get_optimizer(\n",
        "            self.config.optimizer, self.config.optimizer_params, self.config.lr_disc, self.model_d\n",
        "        )\n",
        "        return [optimizer2, optimizer1]\n",
        "\n",
        "    def get_lr(self) -> List:\n",
        "        return [self.config.lr_disc, self.config.lr_gen]\n",
        "\n",
        "    def get_scheduler(self, optimizer) -> List:\n",
        "        scheduler1 = get_scheduler(self.config.lr_scheduler_gen, self.config.lr_scheduler_gen_params, optimizer[0])\n",
        "        scheduler2 = get_scheduler(self.config.lr_scheduler_disc, self.config.lr_scheduler_disc_params, optimizer[1])\n",
        "        return [scheduler2, scheduler1]\n",
        "\n",
        "    @staticmethod\n",
        "    def format_batch(batch: List) -> Dict:\n",
        "        if isinstance(batch[0], list):\n",
        "            x_G, y_G = batch[0]\n",
        "            x_D, y_D = batch[1]\n",
        "            return {\"input\": x_G, \"waveform\": y_G, \"input_disc\": x_D, \"waveform_disc\": y_D}\n",
        "        x, y = batch\n",
        "        return {\"input\": x, \"waveform\": y}\n",
        "\n",
        "    def get_data_loader(\n",
        "        self,\n",
        "        config: Coqpit,\n",
        "        assets: Dict,\n",
        "        is_eval: True,\n",
        "        samples: List,\n",
        "        verbose: bool,\n",
        "        num_gpus: int,\n",
        "        rank: int = None,\n",
        "    ):\n",
        "        dataset = GANDataset(\n",
        "            ap=self.ap,\n",
        "            items=samples,\n",
        "            seq_len=config.seq_len,\n",
        "            hop_len=self.ap.hop_length,\n",
        "            pad_short=config.pad_short,\n",
        "            conv_pad=config.conv_pad,\n",
        "            return_pairs=config.diff_samples_for_G_and_D if \"diff_samples_for_G_and_D\" in config else False,\n",
        "            is_training=not is_eval,\n",
        "            return_segments=not is_eval,\n",
        "            use_noise_augment=config.use_noise_augment,\n",
        "            use_cache=config.use_cache,\n",
        "            verbose=verbose,\n",
        "        )\n",
        "        dataset.shuffle_mapping()\n",
        "        sampler = DistributedSampler(dataset, shuffle=True) if num_gpus > 1 else None\n",
        "        loader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=1 if is_eval else config.batch_size,\n",
        "            shuffle=num_gpus == 0,\n",
        "            drop_last=False,\n",
        "            sampler=sampler,\n",
        "            num_workers=config.num_eval_loader_workers if is_eval else config.num_loader_workers,\n",
        "            pin_memory=False,\n",
        "        )\n",
        "        return loader\n",
        "\n",
        "    def get_criterion(self):\n",
        "        return [DiscriminatorLoss(self.config), GeneratorLoss(self.config)]\n",
        "\n",
        "    @staticmethod\n",
        "    def init_from_config(config: Coqpit, verbose=True) -> \"GAN\":\n",
        "        ap = AudioProcessor.init_from_config(config, verbose=verbose)\n",
        "        return GAN(config, ap=ap)"
      ],
      "metadata": {
        "id": "aMR0718jJC2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class GBlock(nn.Module):\n",
        "    def __init__(self, in_channels, cond_channels, downsample_factor):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.cond_channels = cond_channels\n",
        "        self.downsample_factor = downsample_factor\n",
        "        self.start = nn.Sequential(\n",
        "            nn.AvgPool1d(downsample_factor, stride=downsample_factor),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(in_channels, in_channels * 2, kernel_size=3, padding=1),\n",
        "        )\n",
        "        self.lc_conv1d = nn.Conv1d(cond_channels, in_channels * 2, kernel_size=1)\n",
        "        self.end = nn.Sequential(\n",
        "            nn.ReLU(), nn.Conv1d(in_channels * 2, in_channels * 2, kernel_size=3, dilation=2, padding=2)\n",
        "        )\n",
        "        self.residual = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, in_channels * 2, kernel_size=1),\n",
        "            nn.AvgPool1d(downsample_factor, stride=downsample_factor),\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs, conditions):\n",
        "        outputs = self.start(inputs) + self.lc_conv1d(conditions)\n",
        "        outputs = self.end(outputs)\n",
        "        residual_outputs = self.residual(inputs)\n",
        "        outputs = outputs + residual_outputs\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class DBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downsample_factor):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.downsample_factor = downsample_factor\n",
        "        self.out_channels = out_channels\n",
        "        self.donwsample_layer = nn.AvgPool1d(downsample_factor, stride=downsample_factor)\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(out_channels, out_channels, kernel_size=3, dilation=2, padding=2),\n",
        "        )\n",
        "        self.residual = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels, kernel_size=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if self.downsample_factor > 1:\n",
        "            outputs = self.layers(self.donwsample_layer(inputs)) + self.donwsample_layer(self.residual(inputs))\n",
        "        else:\n",
        "            outputs = self.layers(inputs) + self.residual(inputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class ConditionalDiscriminator(nn.Module):\n",
        "    def __init__(self, in_channels, cond_channels, downsample_factors=(2, 2, 2), out_channels=(128, 256)):\n",
        "        super().__init__()\n",
        "        assert len(downsample_factors) == len(out_channels) + 1\n",
        "        self.in_channels = in_channels\n",
        "        self.cond_channels = cond_channels\n",
        "        self.downsample_factors = downsample_factors\n",
        "        self.out_channels = out_channels\n",
        "        self.pre_cond_layers = nn.ModuleList()\n",
        "        self.post_cond_layers = nn.ModuleList()\n",
        "        self.pre_cond_layers += [DBlock(in_channels, 64, 1)]\n",
        "        in_channels = 64\n",
        "        for i, channel in enumerate(out_channels):\n",
        "            self.pre_cond_layers.append(DBlock(in_channels, channel, downsample_factors[i]))\n",
        "            in_channels = channel\n",
        "        self.cond_block = GBlock(in_channels, cond_channels, downsample_factors[-1])\n",
        "        self.post_cond_layers += [\n",
        "            DBlock(in_channels * 2, in_channels * 2, 1),\n",
        "            DBlock(in_channels * 2, in_channels * 2, 1),\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Conv1d(in_channels * 2, 1, kernel_size=1),\n",
        "        ]\n",
        "\n",
        "    def forward(self, inputs, conditions):\n",
        "        batch_size = inputs.size()[0]\n",
        "        outputs = inputs.view(batch_size, self.in_channels, -1)\n",
        "        for layer in self.pre_cond_layers:\n",
        "            outputs = layer(outputs)\n",
        "        outputs = self.cond_block(outputs, conditions)\n",
        "        for layer in self.post_cond_layers:\n",
        "            outputs = layer(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class UnconditionalDiscriminator(nn.Module):\n",
        "    def __init__(self, in_channels, base_channels=64, downsample_factors=(8, 4), out_channels=(128, 256)):\n",
        "        super().__init__()\n",
        "        self.downsample_factors = downsample_factors\n",
        "        self.in_channels = in_channels\n",
        "        self.downsample_factors = downsample_factors\n",
        "        self.out_channels = out_channels\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers += [DBlock(self.in_channels, base_channels, 1)]\n",
        "        in_channels = base_channels\n",
        "        for i, factor in enumerate(downsample_factors):\n",
        "            self.layers.append(DBlock(in_channels, out_channels[i], factor))\n",
        "            in_channels *= 2\n",
        "        self.layers += [\n",
        "            DBlock(in_channels, in_channels, 1),\n",
        "            DBlock(in_channels, in_channels, 1),\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Conv1d(in_channels, 1, kernel_size=1),\n",
        "        ]\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        batch_size = inputs.size()[0]\n",
        "        outputs = inputs.view(batch_size, self.in_channels, -1)\n",
        "        for layer in self.layers:\n",
        "            outputs = layer(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class RandomWindowDiscriminator(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        cond_channels,\n",
        "        hop_length,\n",
        "        uncond_disc_donwsample_factors=(8, 4),\n",
        "        cond_disc_downsample_factors=((8, 4, 2, 2, 2), (8, 4, 2, 2), (8, 4, 2), (8, 4), (4, 2, 2)),\n",
        "        cond_disc_out_channels=((128, 128, 256, 256), (128, 256, 256), (128, 256), (256,), (128, 256)),\n",
        "        window_sizes=(512, 1024, 2048, 4096, 8192),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.cond_channels = cond_channels\n",
        "        self.window_sizes = window_sizes\n",
        "        self.hop_length = hop_length\n",
        "        self.base_window_size = self.hop_length * 2\n",
        "        self.ks = [ws // self.base_window_size for ws in window_sizes]\n",
        "        assert len(cond_disc_downsample_factors) == len(cond_disc_out_channels) == len(window_sizes)\n",
        "        for ws in window_sizes:\n",
        "            assert ws % hop_length == 0\n",
        "        for idx, cf in enumerate(cond_disc_downsample_factors):\n",
        "            assert np.prod(cf) == hop_length // self.ks[idx]\n",
        "        self.unconditional_discriminators = nn.ModuleList([])\n",
        "        for k in self.ks:\n",
        "            layer = UnconditionalDiscriminator(\n",
        "                in_channels=k, base_channels=64, downsample_factors=uncond_disc_donwsample_factors\n",
        "            )\n",
        "            self.unconditional_discriminators.append(layer)\n",
        "        self.conditional_discriminators = nn.ModuleList([])\n",
        "        for idx, k in enumerate(self.ks):\n",
        "            layer = ConditionalDiscriminator(\n",
        "                in_channels=k,\n",
        "                cond_channels=cond_channels,\n",
        "                downsample_factors=cond_disc_downsample_factors[idx],\n",
        "                out_channels=cond_disc_out_channels[idx],\n",
        "            )\n",
        "            self.conditional_discriminators.append(layer)\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        scores = []\n",
        "        feats = []\n",
        "        for window_size, layer in zip(self.window_sizes, self.unconditional_discriminators):\n",
        "            index = np.random.randint(x.shape[-1] - window_size)\n",
        "            score = layer(x[:, :, index : index + window_size])\n",
        "            scores.append(score)\n",
        "        for window_size, layer in zip(self.window_sizes, self.conditional_discriminators):\n",
        "            frame_size = window_size // self.hop_length\n",
        "            lc_index = np.random.randint(c.shape[-1] - frame_size)\n",
        "            sample_index = lc_index * self.hop_length\n",
        "            x_sub = x[:, :, sample_index : (lc_index + frame_size) * self.hop_length]\n",
        "            c_sub = c[:, :, lc_index : lc_index + frame_size]\n",
        "            score = layer(x_sub, c_sub)\n",
        "            scores.append(score)\n",
        "        return scores, feats"
      ],
      "metadata": {
        "id": "tq1mHVFyKB6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.nn.utils.parametrizations import weight_norm\n",
        "from torch.nn.utils.parametrize import remove_parametrizations\n",
        "\n",
        "class ResidualStack(nn.Module):\n",
        "    def __init__(self, channels, num_res_blocks, kernel_size):\n",
        "        super().__init__()\n",
        "\n",
        "        assert (kernel_size - 1) % 2 == 0, \" [!] kernel_size has to be odd.\"\n",
        "        base_padding = (kernel_size - 1) // 2\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "        for idx in range(num_res_blocks):\n",
        "            layer_kernel_size = kernel_size\n",
        "            layer_dilation = layer_kernel_size**idx\n",
        "            layer_padding = base_padding * layer_dilation\n",
        "            self.blocks += [\n",
        "                nn.Sequential(\n",
        "                    nn.LeakyReLU(0.2),\n",
        "                    nn.ReflectionPad1d(layer_padding),\n",
        "                    weight_norm(\n",
        "                        nn.Conv1d(channels, channels, kernel_size=kernel_size, dilation=layer_dilation, bias=True)\n",
        "                    ),\n",
        "                    nn.LeakyReLU(0.2),\n",
        "                    weight_norm(nn.Conv1d(channels, channels, kernel_size=1, bias=True)),\n",
        "                )\n",
        "            ]\n",
        "\n",
        "        self.shortcuts = nn.ModuleList(\n",
        "            [weight_norm(nn.Conv1d(channels, channels, kernel_size=1, bias=True)) for _ in range(num_res_blocks)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for block, shortcut in zip(self.blocks, self.shortcuts):\n",
        "            x = shortcut(x) + block(x)\n",
        "        return x\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        for block, shortcut in zip(self.blocks, self.shortcuts):\n",
        "            remove_parametrizations(block[2], \"weight\")\n",
        "            remove_parametrizations(block[4], \"weight\")\n",
        "            remove_parametrizations(shortcut, \"weight\")\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class GroupNorm32(nn.GroupNorm):\n",
        "    def forward(self, x):\n",
        "        return super().forward(x.float()).type(x.dtype)\n",
        "\n",
        "def conv_nd(dims, *args, **kwargs):\n",
        "    if dims == 1:\n",
        "        return nn.Conv1d(*args, **kwargs)\n",
        "    elif dims == 2:\n",
        "        return nn.Conv2d(*args, **kwargs)\n",
        "    elif dims == 3:\n",
        "        return nn.Conv3d(*args, **kwargs)\n",
        "    raise ValueError(f\"unsupported dimensions: {dims}\")\n",
        "\n",
        "def normalization(channels):\n",
        "    groups = 32\n",
        "    if channels <= 16:\n",
        "        groups = 8\n",
        "    elif channels <= 64:\n",
        "        groups = 16\n",
        "    while channels % groups != 0:\n",
        "        groups = int(groups / 2)\n",
        "    assert groups > 2\n",
        "    return GroupNorm32(groups, channels)\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class QKVAttention(nn.Module):\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv, mask=None, qk_bias=0):\n",
        "        bs, width, length = qkv.shape\n",
        "        assert width % (3 * self.n_heads) == 0\n",
        "        ch = width // (3 * self.n_heads)\n",
        "        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n",
        "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "        weight = torch.einsum(\"bct,bcs->bts\", q * scale, k * scale)\n",
        "        weight = weight + qk_bias\n",
        "        if mask is not None:\n",
        "            mask = mask.repeat(self.n_heads, 1, 1)\n",
        "            weight[mask.logical_not()] = -torch.inf\n",
        "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = torch.einsum(\"bts,bcs->bct\", weight, v)\n",
        "        return a.reshape(bs, -1, length)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        num_heads=1,\n",
        "        num_head_channels=-1,\n",
        "        out_channels=None,\n",
        "        do_activation=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        out_channels = channels if out_channels is None else out_channels\n",
        "        self.do_activation = do_activation\n",
        "        if num_head_channels == -1:\n",
        "            self.num_heads = num_heads\n",
        "        else:\n",
        "            assert (\n",
        "                channels % num_head_channels == 0\n",
        "            ), f\"q,k,v channels {channels} is not divisible by num_head_channels {num_head_channels}\"\n",
        "            self.num_heads = channels // num_head_channels\n",
        "        self.norm = normalization(channels)\n",
        "        self.qkv = conv_nd(1, channels, out_channels * 3, 1)\n",
        "        self.attention = QKVAttention(self.num_heads)\n",
        "        self.x_proj = nn.Identity() if out_channels == channels else conv_nd(1, channels, out_channels, 1)\n",
        "        self.proj_out = zero_module(conv_nd(1, out_channels, out_channels, 1))\n",
        "\n",
        "    def forward(self, x, mask=None, qk_bias=0):\n",
        "        b, c, *spatial = x.shape\n",
        "        if mask is not None:\n",
        "            if len(mask.shape) == 2:\n",
        "                mask = mask.unsqueeze(0).repeat(x.shape[0], 1, 1)\n",
        "            if mask.shape[1] != x.shape[-1]:\n",
        "                mask = mask[:, : x.shape[-1], : x.shape[-1]]\n",
        "        x = x.reshape(b, c, -1)\n",
        "        x = self.norm(x)\n",
        "        if self.do_activation:\n",
        "            x = F.silu(x, inplace=True)\n",
        "        qkv = self.qkv(x)\n",
        "        h = self.attention(qkv, mask=mask, qk_bias=qk_bias)\n",
        "        h = self.proj_out(h)\n",
        "        xp = self.x_proj(x)\n",
        "        return (xp + h).reshape(b, xp.shape[1], *spatial)\n",
        "\n",
        "class ConditioningEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        spec_dim,\n",
        "        embedding_dim,\n",
        "        attn_blocks=6,\n",
        "        num_attn_heads=4,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        attn = []\n",
        "        self.init = nn.Conv1d(spec_dim, embedding_dim, kernel_size=1)\n",
        "        for a in range(attn_blocks):\n",
        "            attn.append(AttentionBlock(embedding_dim, num_attn_heads))\n",
        "        self.attn = nn.Sequential(*attn)\n",
        "        self.dim = embedding_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.init(x)\n",
        "        h = self.attn(h)\n",
        "        return h"
      ],
      "metadata": {
        "id": "ITQSHCzvKpcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from coqpit import Coqpit\n",
        "\n",
        "from TTS.tts.layers.vcvc.nlp import NLP\n",
        "from TTS.tts.layers.vcvc.hifigan_decoder import HifiDecoder\n",
        "from TTS.tts.layers.vcvc.stream_generator import init_stream_support\n",
        "from TTS.tts.layers.vcvc.tokenizer import VoiceBpeTokenizer, split_sentence\n",
        "from TTS.tts.layers.vcvc.vcvc_manager import SpeakerManager, LanguageManager\n",
        "from TTS.tts.models.base_tts import BaseTTS\n",
        "from TTS.utils.io import load_fsspec\n",
        "\n",
        "init_stream_support()\n",
        "\n",
        "def wav_to_mel_cloning(\n",
        "    wav,\n",
        "    mel_norms_file=\"...pth\",\n",
        "    mel_norms=None,\n",
        "    device=torch.device(\"cpu\"),\n",
        "    n_fft=4096,\n",
        "    hop_length=1024,\n",
        "    win_length=4096,\n",
        "    power=2,\n",
        "    normalized=False,\n",
        "    sample_rate=22050,\n",
        "    f_min=0,\n",
        "    f_max=8000,\n",
        "    n_mels=80,\n",
        "):\n",
        "    mel_stft = torchaudio.transforms.MelSpectrogram(\n",
        "        n_fft=n_fft,\n",
        "        hop_length=hop_length,\n",
        "        win_length=win_length,\n",
        "        power=power,\n",
        "        normalized=normalized,\n",
        "        sample_rate=sample_rate,\n",
        "        f_min=f_min,\n",
        "        f_max=f_max,\n",
        "        n_mels=n_mels,\n",
        "        norm=\"slaney\",\n",
        "    ).to(device)\n",
        "    wav = wav.to(device)\n",
        "    mel = mel_stft(wav)\n",
        "    mel = torch.log(torch.clamp(mel, min=1e-5))\n",
        "    if mel_norms is None:\n",
        "        mel_norms = torch.load(mel_norms_file, map_location=device)\n",
        "    mel = mel / mel_norms.unsqueeze(0).unsqueeze(-1)\n",
        "    return mel\n",
        "\n",
        "def load_audio(audiopath, sampling_rate):\n",
        "    audio, lsr = torchaudio.load(audiopath)\n",
        "    if audio.size(0) != 1:\n",
        "        audio = torch.mean(audio, dim=0, keepdim=True)\n",
        "\n",
        "    if lsr != sampling_rate:\n",
        "        audio = torchaudio.functional.resample(audio, lsr, sampling_rate)\n",
        "    if torch.any(audio > 10) or not torch.any(audio < 0):\n",
        "        print(f\"Error with {audiopath}. Max={audio.max()} min={audio.min()}\")\n",
        "    audio.clip_(-1, 1)\n",
        "    return audio\n",
        "\n",
        "def pad_or_truncate(t, length):\n",
        "    tp = t[..., :length]\n",
        "    if t.shape[-1] == length:\n",
        "        tp = t\n",
        "    elif t.shape[-1] < length:\n",
        "        tp = F.pad(t, (0, length - t.shape[-1]))\n",
        "    return tp\n",
        "\n",
        "@dataclass\n",
        "class VcvcAudioConfig(Coqpit):\n",
        "    sample_rate: int = 22050\n",
        "    output_sample_rate: int = 24000\n",
        "\n",
        "@dataclass\n",
        "class VcvcArgs(Coqpit):\n",
        "    nlp_batch_size: int = 1\n",
        "    enable_redaction: bool = False\n",
        "    kv_cache: bool = True\n",
        "    nlp_checkpoint: str = None\n",
        "    clvp_checkpoint: str = None\n",
        "    decoder_checkpoint: str = None\n",
        "    num_chars: int = 255\n",
        "    tokenizer_file: str = \"\"\n",
        "    nlp_max_audio_tokens: int = 605\n",
        "    nlp_max_text_tokens: int = 402\n",
        "    nlp_max_prompt_tokens: int = 70\n",
        "    nlp_layers: int = 30\n",
        "    nlp_n_model_channels: int = 1024\n",
        "    nlp_n_heads: int = 16\n",
        "    nlp_number_text_tokens: int = None\n",
        "    nlp_start_text_token: int = None\n",
        "    nlp_stop_text_token: int = None\n",
        "    nlp_num_audio_tokens: int = 8194\n",
        "    nlp_start_audio_token: int = 8192\n",
        "    nlp_stop_audio_token: int = 8193\n",
        "    nlp_code_stride_len: int = 1024\n",
        "    nlp_use_masking_gt_prompt_approach: bool = True\n",
        "    nlp_use_perceiver_resampler: bool = False\n",
        "\n",
        "    input_sample_rate: int = 22050\n",
        "    output_sample_rate: int = 24000\n",
        "    output_hop_length: int = 256\n",
        "    decoder_input_dim: int = 1024\n",
        "    d_vector_dim: int = 512\n",
        "    cond_d_vector_in_each_upsampling_layer: bool = True\n",
        "\n",
        "    duration_const: int = 102400\n",
        "\n",
        "class Vcvc(BaseTTS):\n",
        "    def __init__(self, config: Coqpit):\n",
        "        super().__init__(config, ap=None, tokenizer=None)\n",
        "        self.mel_stats_path = None\n",
        "        self.config = config\n",
        "        self.nlp_checkpoint = self.args.nlp_checkpoint\n",
        "        self.decoder_checkpoint = self.args.decoder_checkpoint\n",
        "        self.models_dir = config.model_dir\n",
        "        self.nlp_batch_size = self.args.nlp_batch_size\n",
        "\n",
        "        self.tokenizer = VoiceBpeTokenizer()\n",
        "        self.nlp = None\n",
        "        self.init_models()\n",
        "        self.register_buffer(\"mel_stats\", torch.ones(80))\n",
        "\n",
        "    def init_models(self):\n",
        "        if self.tokenizer.tokenizer is not None:\n",
        "            self.args.nlp_number_text_tokens = self.tokenizer.get_number_tokens()\n",
        "            self.args.nlp_start_text_token = self.tokenizer.tokenizer.token_to_id(\"[START]\")\n",
        "            self.args.nlp_stop_text_token = self.tokenizer.tokenizer.token_to_id(\"[STOP]\")\n",
        "\n",
        "        if self.args.nlp_number_text_tokens:\n",
        "            self.nlp = NLP(\n",
        "                layers=self.args.nlp_layers,\n",
        "                model_dim=self.args.nlp_n_model_channels,\n",
        "                start_text_token=self.args.nlp_start_text_token,\n",
        "                stop_text_token=self.args.nlp_stop_text_token,\n",
        "                heads=self.args.nlp_n_heads,\n",
        "                max_text_tokens=self.args.nlp_max_text_tokens,\n",
        "                max_mel_tokens=self.args.nlp_max_audio_tokens,\n",
        "                max_prompt_tokens=self.args.nlp_max_prompt_tokens,\n",
        "                number_text_tokens=self.args.nlp_number_text_tokens,\n",
        "                num_audio_tokens=self.args.nlp_num_audio_tokens,\n",
        "                start_audio_token=self.args.nlp_start_audio_token,\n",
        "                stop_audio_token=self.args.nlp_stop_audio_token,\n",
        "                use_perceiver_resampler=self.args.nlp_use_perceiver_resampler,\n",
        "                code_stride_len=self.args.nlp_code_stride_len,\n",
        "            )\n",
        "\n",
        "        self.hifigan_decoder = HifiDecoder(\n",
        "            input_sample_rate=self.args.input_sample_rate,\n",
        "            output_sample_rate=self.args.output_sample_rate,\n",
        "            output_hop_length=self.args.output_hop_length,\n",
        "            ar_mel_length_compression=self.args.nlp_code_stride_len,\n",
        "            decoder_input_dim=self.args.decoder_input_dim,\n",
        "            d_vector_dim=self.args.d_vector_dim,\n",
        "            cond_d_vector_in_each_upsampling_layer=self.args.cond_d_vector_in_each_upsampling_layer,\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return next(self.parameters()).device\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def get_nlp_cond_latents(self, audio, sr, length: int = 30, chunk_length: int = 6):\n",
        "        if sr != 22050:\n",
        "            audio = torchaudio.functional.resample(audio, sr, 22050)\n",
        "        if length > 0:\n",
        "            audio = audio[:, : 22050 * length]\n",
        "        if self.args.nlp_use_perceiver_resampler:\n",
        "            style_embs = []\n",
        "            for i in range(0, audio.shape[1], 22050 * chunk_length):\n",
        "                audio_chunk = audio[:, i : i + 22050 * chunk_length]\n",
        "\n",
        "                if audio_chunk.size(-1) < 22050 * 0.33:\n",
        "                    continue\n",
        "\n",
        "                mel_chunk = wav_to_mel_cloning(\n",
        "                    audio_chunk,\n",
        "                    mel_norms=self.mel_stats.cpu(),\n",
        "                    n_fft=2048,\n",
        "                    hop_length=256,\n",
        "                    win_length=1024,\n",
        "                    power=2,\n",
        "                    normalized=False,\n",
        "                    sample_rate=22050,\n",
        "                    f_min=0,\n",
        "                    f_max=8000,\n",
        "                    n_mels=80,\n",
        "                )\n",
        "                style_emb = self.nlp.get_style_emb(mel_chunk.to(self.device), None)\n",
        "                style_embs.append(style_emb)\n",
        "\n",
        "            cond_latent = torch.stack(style_embs).mean(dim=0)\n",
        "        else:\n",
        "            mel = wav_to_mel_cloning(\n",
        "                audio,\n",
        "                mel_norms=self.mel_stats.cpu(),\n",
        "                n_fft=4096,\n",
        "                hop_length=1024,\n",
        "                win_length=4096,\n",
        "                power=2,\n",
        "                normalized=False,\n",
        "                sample_rate=22050,\n",
        "                f_min=0,\n",
        "                f_max=8000,\n",
        "                n_mels=80,\n",
        "            )\n",
        "            cond_latent = self.nlp.get_style_emb(mel.to(self.device))\n",
        "        return cond_latent.transpose(1, 2)\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def get_speaker_embedding(self, audio, sr):\n",
        "        audio_16k = torchaudio.functional.resample(audio, sr, 16000)\n",
        "        return (\n",
        "            self.hifigan_decoder.speaker_encoder.forward(audio_16k.to(self.device), l2_norm=True)\n",
        "            .unsqueeze(-1)\n",
        "            .to(self.device)\n",
        "        )\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def get_conditioning_latents(\n",
        "        self,\n",
        "        audio_path,\n",
        "        max_ref_length=30,\n",
        "        nlp_cond_len=6,\n",
        "        nlp_cond_chunk_len=6,\n",
        "        librosa_trim_db=None,\n",
        "        sound_norm_refs=False,\n",
        "        load_sr=22050,\n",
        "    ):\n",
        "        if not isinstance(audio_path, list):\n",
        "            audio_paths = [audio_path]\n",
        "        else:\n",
        "            audio_paths = audio_path\n",
        "\n",
        "        speaker_embeddings = []\n",
        "        audios = []\n",
        "        speaker_embedding = None\n",
        "        for file_path in audio_paths:\n",
        "            audio = load_audio(file_path, load_sr)\n",
        "            audio = audio[:, : load_sr * max_ref_length].to(self.device)\n",
        "            if sound_norm_refs:\n",
        "                audio = (audio / torch.abs(audio).max()) * 0.75\n",
        "            if librosa_trim_db is not None:\n",
        "                audio = librosa.effects.trim(audio, top_db=librosa_trim_db)[0]\n",
        "\n",
        "            speaker_embedding = self.get_speaker_embedding(audio, load_sr)\n",
        "            speaker_embeddings.append(speaker_embedding)\n",
        "\n",
        "            audios.append(audio)\n",
        "\n",
        "        full_audio = torch.cat(audios, dim=-1)\n",
        "        nlp_cond_latents = self.get_nlp_cond_latents(\n",
        "            full_audio, load_sr, length=nlp_cond_len, chunk_length=nlp_cond_chunk_len\n",
        "        )\n",
        "\n",
        "        if speaker_embeddings:\n",
        "            speaker_embedding = torch.stack(speaker_embeddings)\n",
        "            speaker_embedding = speaker_embedding.mean(dim=0)\n",
        "\n",
        "        return nlp_cond_latents, speaker_embedding\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def eval_step(self):\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "YNzal0xHNXFA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}