{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "if \"../\" not in sys.path:\n",
        "    sys.path.append(\"../\")\n",
        "from lib.envs.gridworld import GridworldEnv\n",
        "\n",
        "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
        "    V = np.zeros(env.nS)\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(env.nS):\n",
        "            v = 0\n",
        "            for a, action_prob in enumerate(policy[s]):\n",
        "                for prob, next_state, reward, done in env.P[s][a]:\n",
        "                    v += action_prob * prob * (reward + discount_factor * V[next_state])\n",
        "            delta = max(delta, np.abs(v - V[s]))\n",
        "            V[s] = v\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return np.array(V)\n",
        "\n",
        "def policy_improvement(env, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
        "\n",
        "    def one_step_lookahead(state, V):\n",
        "        A = np.zeros(env.nA)\n",
        "        for a in range(env.nA):\n",
        "            for prob, next_state, reward, done in env.P[state][a]:\n",
        "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
        "        return A\n",
        "\n",
        "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
        "\n",
        "    while True:\n",
        "        V = policy_eval_fn(policy, env, discount_factor)\n",
        "        policy_stable = True\n",
        "\n",
        "        for s in range(env.nS):\n",
        "            chosen_a = np.argmax(policy[s])\n",
        "            action_values = one_step_lookahead(s, V)\n",
        "            best_a = np.argmax(action_values)\n",
        "\n",
        "            if chosen_a != best_a:\n",
        "                policy_stable = False\n",
        "            policy[s] = np.eye(env.nA)[best_a]\n",
        "\n",
        "        if policy_stable:\n",
        "            return policy, V"
      ],
      "metadata": {
        "id": "vsOalzc6NGjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "from lib.envs.gridworld import GridworldEnv\n",
        "\n",
        "env = GridworldEnv()\n",
        "\n",
        "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
        "\n",
        "    def one_step_lookahead(state, V):\n",
        "        A = np.zeros(env.nA)\n",
        "        for a in range(env.nA):\n",
        "            for prob, next_state, reward, done in env.P[state][a]:\n",
        "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
        "        return A\n",
        "\n",
        "    V = np.zeros(env.nS)\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(env.nS):\n",
        "            A = one_step_lookahead(s, V)\n",
        "            best_action_value = np.max(A)\n",
        "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
        "            V[s] = best_action_value\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    policy = np.zeros([env.nS, env.nA])\n",
        "    for s in range(env.nS):\n",
        "        A = one_step_lookahead(s, V)\n",
        "        best_action = np.argmax(A)\n",
        "        policy[s, best_action] = 1.0\n",
        "\n",
        "    return policy, V"
      ],
      "metadata": {
        "id": "FzLO3iTCOfE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "from lib.envs.blackjack import BlackjackEnv\n",
        "from lib import plotting\n",
        "\n",
        "matplotlib.style.use('ggplot')\n",
        "env = BlackjackEnv()\n",
        "\n",
        "def mc_prediction(policy, env, num_episodes, discount_factor=1.0):\n",
        "    returns_sum = defaultdict(float)\n",
        "    returns_count = defaultdict(float)\n",
        "    V = defaultdict(float)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        if i_episode % 1000 == 0:\n",
        "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        episode = []\n",
        "        state = env.reset()\n",
        "        for t in range(100):\n",
        "            action = policy(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            if done:\n",
        "                break\n",
        "            state = next_state\n",
        "\n",
        "        states_in_episode = set([tuple(x[0]) for x in episode])\n",
        "        for state in states_in_episode:\n",
        "            first_occurence_idx = next(i for i,x in enumerate(episode) if x[0] == state)\n",
        "            G = sum([x[2]*(discount_factor**i) for i,x in enumerate(episode[first_occurence_idx:])])\n",
        "            returns_sum[state] += G\n",
        "            returns_count[state] += 1.0\n",
        "            V[state] = returns_sum[state] / returns_count[state]\n",
        "\n",
        "    return V\n",
        "\n",
        "def sample_policy(observation):\n",
        "    score, dealer_score, usable_ace = observation\n",
        "    return 0 if score >= 20 else 1"
      ],
      "metadata": {
        "id": "fip9c8X42n58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "if \"../\" not in sys.path:\n",
        "  sys.path.append(\"../\")\n",
        "from lib.envs.blackjack import BlackjackEnv\n",
        "from lib import plotting\n",
        "\n",
        "matplotlib.style.use('ggplot')\n",
        "env = BlackjackEnv()\n",
        "\n",
        "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
        "    def policy_fn(observation):\n",
        "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
        "        best_action = np.argmax(Q[observation])\n",
        "        A[best_action] += (1.0 - epsilon)\n",
        "        return A\n",
        "    return policy_fn\n",
        "\n",
        "def mc_control_epsilon_greedy(env, num_episodes, discount_factor=1.0, epsilon=0.1):\n",
        "    returns_sum = defaultdict(float)\n",
        "    returns_count = defaultdict(float)\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        if i_episode % 1000 == 0:\n",
        "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        episode = []\n",
        "        state = env.reset()\n",
        "        for t in range(100):\n",
        "            probs = policy(state)\n",
        "            action = np.random.choice(np.arange(len(probs)), p=probs)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            if done:\n",
        "                break\n",
        "            state = next_state\n",
        "\n",
        "        sa_in_episode = set([(tuple(x[0]), x[1]) for x in episode])\n",
        "        for state, action in sa_in_episode:\n",
        "            sa_pair = (state, action)\n",
        "            first_occurence_idx = next(i for i,x in enumerate(episode)\n",
        "                                       if x[0] == state and x[1] == action)\n",
        "            G = sum([x[2]*(discount_factor**i) for i,x in enumerate(episode[first_occurence_idx:])])\n",
        "            returns_sum[sa_pair] += G\n",
        "            returns_count[sa_pair] += 1.0\n",
        "            Q[state][action] = returns_sum[sa_pair] / returns_count[sa_pair]\n",
        "\n",
        "    return Q, policy\n",
        "\n",
        "Q, policy = mc_control_epsilon_greedy(env, num_episodes=500000, epsilon=0.1)"
      ],
      "metadata": {
        "id": "hyLyEm2C2tpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "if \"../\" not in sys.path:\n",
        "  sys.path.append(\"../\")\n",
        "from lib.envs.blackjack import BlackjackEnv\n",
        "from lib import plotting\n",
        "\n",
        "matplotlib.style.use('ggplot')\n",
        "env = BlackjackEnv()\n",
        "\n",
        "def create_random_policy(nA):\n",
        "    A = np.ones(nA, dtype=float) / nA\n",
        "    def policy_fn(observation):\n",
        "        return A\n",
        "    return policy_fn\n",
        "\n",
        "def create_greedy_policy(Q):\n",
        "    def policy_fn(state):\n",
        "        A = np.zeros_like(Q[state], dtype=float)\n",
        "        best_action = np.argmax(Q[state])\n",
        "        A[best_action] = 1.0\n",
        "        return A\n",
        "    return policy_fn\n",
        "\n",
        "def mc_control_importance_sampling(env, num_episodes, behavior_policy, discount_factor=1.0):\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    C = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    target_policy = create_greedy_policy(Q)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        if i_episode % 1000 == 0:\n",
        "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        episode = []\n",
        "        state = env.reset()\n",
        "        for t in range(100):\n",
        "            probs = behavior_policy(state)\n",
        "            action = np.random.choice(np.arange(len(probs)), p=probs)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            if done:\n",
        "                break\n",
        "            state = next_state\n",
        "\n",
        "        G = 0.0\n",
        "        W = 1.0\n",
        "        for t in range(len(episode))[::-1]:\n",
        "            state, action, reward = episode[t]\n",
        "            G = discount_factor * G + reward\n",
        "            C[state][action] += W\n",
        "            Q[state][action] += (W / C[state][action]) * (G - Q[state][action])\n",
        "            if action !=  np.argmax(target_policy(state)):\n",
        "                break\n",
        "            W = W * 1./behavior_policy(state)[action]\n",
        "\n",
        "    return Q, target_policy\n",
        "\n",
        "random_policy = create_random_policy(env.action_space.n)\n",
        "Q, policy = mc_control_importance_sampling(env, num_episodes=500000, behavior_policy=random_policy)"
      ],
      "metadata": {
        "id": "SvJKzhS23Gzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "BOARD_ROWS = 5\n",
        "BOARD_COLS = 5\n",
        "START = (0, 0)\n",
        "WIN_STATE = (4, 4)\n",
        "HOLE_STATE = [(1,0),(3,1),(4,2),(1,3)]\n",
        "\n",
        "class State:\n",
        "    def __init__(self, state=START):\n",
        "        self.state = state\n",
        "        self.isEnd = False\n",
        "\n",
        "    def getReward(self):\n",
        "        for i in HOLE_STATE:\n",
        "            if self.state == i:\n",
        "                return -5\n",
        "        if self.state == WIN_STATE:\n",
        "            return 1\n",
        "        else:\n",
        "            return -1\n",
        "\n",
        "    def isEndFunc(self):\n",
        "        if (self.state == WIN_STATE):\n",
        "            self.isEnd = True\n",
        "        for i in HOLE_STATE:\n",
        "            if self.state == i:\n",
        "                self.isEnd = True\n",
        "\n",
        "    def nxtPosition(self, action):\n",
        "        if action == 0:\n",
        "            nxtState = (self.state[0] - 1, self.state[1]) # up\n",
        "        elif action == 1:\n",
        "            nxtState = (self.state[0] + 1, self.state[1]) # down\n",
        "        elif action == 2:\n",
        "            nxtState = (self.state[0], self.state[1] - 1) # left\n",
        "        else:\n",
        "            nxtState = (self.state[0], self.state[1] + 1) # right\n",
        "\n",
        "        if (nxtState[0] >= 0) and (nxtState[0] <= 4):\n",
        "            if (nxtState[1] >= 0) and (nxtState[1] <= 4):\n",
        "                    return nxtState\n",
        "        return self.state\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.states = []\n",
        "        self.actions = [0,1,2,3]\n",
        "        self.State = State()\n",
        "        self.alpha = 0.5\n",
        "        self.gamma = 0.9\n",
        "        self.epsilon = 0.1\n",
        "        self.isEnd = self.State.isEnd\n",
        "        self.plot_reward = []\n",
        "        self.Q = {}\n",
        "        self.new_Q = {}\n",
        "        self.rewards = 0\n",
        "        for i in range(BOARD_ROWS):\n",
        "            for j in range(BOARD_COLS):\n",
        "                for k in range(len(self.actions)):\n",
        "                    self.Q[(i, j, k)] =0\n",
        "                    self.new_Q[(i, j, k)] = 0\n",
        "\n",
        "    def Action(self):\n",
        "        rnd = random.random()\n",
        "        mx_nxt_reward = -10\n",
        "        action = None\n",
        "\n",
        "        if(rnd >self.epsilon):\n",
        "            for k in self.actions:\n",
        "                i,j = self.State.state\n",
        "                nxt_reward = self.Q[(i,j, k)]\n",
        "                if nxt_reward >= mx_nxt_reward:\n",
        "                    action = k\n",
        "                    mx_nxt_reward = nxt_reward\n",
        "        else:\n",
        "            action = np.random.choice(self.actions)\n",
        "\n",
        "        position = self.State.nxtPosition(action)\n",
        "        return position,action\n",
        "\n",
        "    def Q_Learning(self,episodes):\n",
        "        x = 0\n",
        "        while(x < episodes):\n",
        "            if self.isEnd:\n",
        "                reward = self.State.getReward()\n",
        "                self.rewards += reward\n",
        "                self.plot_reward.append(self.rewards)\n",
        "                i,j = self.State.state\n",
        "                for a in self.actions:\n",
        "                    self.new_Q[(i,j,a)] = round(reward,3)\n",
        "                self.State = State()\n",
        "                self.isEnd = self.State.isEnd\n",
        "                self.rewards = 0\n",
        "                x+=1\n",
        "            else:\n",
        "                mx_nxt_value = -10\n",
        "                next_state, action = self.Action()\n",
        "                i,j = self.State.state\n",
        "                reward = self.State.getReward()\n",
        "                self.rewards +=reward\n",
        "                for a in self.actions:\n",
        "                    nxtStateAction = (next_state[0], next_state[1], a)\n",
        "                    q_value = (1-self.alpha)*self.Q[(i,j,action)] + self.alpha*(reward + self.gamma*self.Q[nxtStateAction])\n",
        "                    if q_value >= mx_nxt_value:\n",
        "                        mx_nxt_value = q_value\n",
        "                self.State = State(state=next_state)\n",
        "                self.State.isEndFunc()\n",
        "                self.isEnd = self.State.isEnd\n",
        "                self.new_Q[(i,j,action)] = round(mx_nxt_value,3)\n",
        "            self.Q = self.new_Q.copy()\n",
        "\n",
        "    def plot(self,episodes):\n",
        "        plt.plot(self.plot_reward)\n",
        "        plt.show()\n",
        "\n",
        "    def showValues(self):\n",
        "        for i in range(0, BOARD_ROWS):\n",
        "            print('-----------------------------------------------')\n",
        "            out = '| '\n",
        "            for j in range(0, BOARD_COLS):\n",
        "                mx_nxt_value = -10\n",
        "                for a in self.actions:\n",
        "                    nxt_value = self.Q[(i,j,a)]\n",
        "                    if nxt_value >= mx_nxt_value:\n",
        "                        mx_nxt_value = nxt_value\n",
        "                out += str(mx_nxt_value).ljust(6) + ' | '\n",
        "            print(out)\n",
        "        print('-----------------------------------------------')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ag = Agent()\n",
        "    episodes = 10000\n",
        "    ag.Q_Learning(episodes)\n",
        "    ag.plot(episodes)\n",
        "    ag.showValues()"
      ],
      "metadata": {
        "id": "T_5z5Ok53V3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, num_states, num_actions, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.q_table = np.zeros((num_states, num_actions))\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.random() < self.exploration_rate:\n",
        "            return random.randint(0, self.num_actions - 1)\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state])\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        best_next_action = np.argmax(self.q_table[next_state])\n",
        "        td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]\n",
        "        td_error = td_target - self.q_table[state][action]\n",
        "        self.q_table[state][action] += self.learning_rate * td_error"
      ],
      "metadata": {
        "id": "8LDacZCL3gDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "from tensorflow.keras import layers, models\n",
        "from scipy.spatial.distance import euclidean\n",
        "from scipy.special import kl_div\n",
        "import heapq\n",
        "\n",
        "class DeepQLearningAgent:\n",
        "    def __init__(self, state_dim, action_dim, learning_rate=0.001, discount_factor=0.99,\n",
        "                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995, batch_size=64,\n",
        "                 replay_buffer_size=10000, target_update_frequency=100):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.batch_size = batch_size\n",
        "        self.target_update_frequency = target_update_frequency\n",
        "\n",
        "        self.replay_buffer = deque(maxlen=replay_buffer_size)\n",
        "        self.target_network = self._build_network()\n",
        "        self.q_network = self._build_network()\n",
        "        self.q_network.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n",
        "\n",
        "        self.goal_state = None  # Goal state for A* search\n",
        "\n",
        "    def _build_network(self):\n",
        "        model = models.Sequential([\n",
        "            layers.Conv2D(32, (3, 3), activation='relu', input_shape=self.state_dim),\n",
        "            layers.MaxPooling2D((2, 2)),\n",
        "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "            layers.MaxPooling2D((2, 2)),\n",
        "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.Dense(self.action_dim)\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_network.set_weights(self.q_network.get_weights())\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_dim)\n",
        "        q_values = self.q_network.predict(state)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def experience_replay(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.replay_buffer, self.batch_size)\n",
        "        states, targets = [], []\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = self.q_network.predict(state)\n",
        "            if done:\n",
        "                target[0][action] = reward\n",
        "            else:\n",
        "                target[0][action] = reward + self.discount_factor * np.amax(self.target_network.predict(next_state)[0])\n",
        "            states.append(state[0])\n",
        "            targets.append(target[0])\n",
        "        self.q_network.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def curiosity_driven_exploration(self, state):\n",
        "        # Implement curiosity-driven exploration strategy (e.g., intrinsic reward based on prediction error)\n",
        "        return self.choose_action(state)\n",
        "\n",
        "    def meta_learning(self, state):\n",
        "        # Implement meta-learning strategy (e.g., adapt learning rate, epsilon, etc., based on past experience)\n",
        "        return self.choose_action(state)\n",
        "\n",
        "    def set_goal_state(self, goal_state):\n",
        "        self.goal_state = goal_state\n",
        "\n",
        "    def deep_a_star_search(self, state):\n",
        "        if not self.goal_state:\n",
        "            return None\n",
        "\n",
        "        open_list = []\n",
        "        closed_set = set()\n",
        "        heapq.heappush(open_list, (0, state, []))  # Priority queue: (f-value, state, path)\n",
        "\n",
        "        while open_list:\n",
        "            f, state, path = heapq.heappop(open_list)\n",
        "            if state == self.goal_state:\n",
        "                return path  # Return the path if the goal state is reached\n",
        "\n",
        "            closed_set.add(state)\n",
        "\n",
        "            for action in range(self.action_dim):\n",
        "                next_state = self.get_next_state(state, action)\n",
        "                if next_state not in closed_set:\n",
        "                    g = len(path) + 1  # Cost from start to next state\n",
        "                    h = self.heuristic(next_state)  # Estimated cost from next state to goal\n",
        "                    f_value = g + h\n",
        "                    heapq.heappush(open_list, (f_value, next_state, path + [action]))\n",
        "\n",
        "        return None  # No path found\n",
        "\n",
        "    def heuristic(self, state):\n",
        "        # Define a heuristic function based on the current state and the goal state\n",
        "        # This heuristic function should estimate the cost of reaching the goal state from the current state\n",
        "        return euclidean(state, self.goal_state)\n",
        "\n",
        "    def train(self, env, episodes):\n",
        "        for episode in range(episodes):\n",
        "            state = env.reset()\n",
        "            state = np.reshape(state, [1, *self.state_dim])\n",
        "            done = False\n",
        "            total_reward = 0\n",
        "            while not done:\n",
        "                # Curiosity-driven exploration\n",
        "                curiosity_action = self.curiosity_driven_exploration(state)\n",
        "                # Meta-learning\n",
        "                meta_action = self.meta_learning(state)\n",
        "                # Deep A* Search\n",
        "                if self.goal_state:\n",
        "                    path = self.deep_a_star_search(state)\n",
        "                    if path:\n",
        "                        deep_a_star_action = path[0]  # Choose the first action in the A* search path\n",
        "                    else:\n",
        "                        deep_a_star_action = None\n",
        "                else:\n",
        "                    deep_a_star_action = None\n",
        "\n",
        "                # Choose action combining curiosity, meta-learning, and Deep A* Search\n",
        "                action = self.choose_action(state)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                total_reward += reward\n",
        "                next_state = np.reshape(next_state, [1, *self.state_dim])\n",
        "                self.remember(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                self.experience_replay()\n",
        "                if episode % self.target_update_frequency == 0:\n",
        "                    self.update_target_network()\n",
        "            self.decay_epsilon()\n",
        "            print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "    def get_next_state(self, state, action):\n",
        "        # Implement this method based on your environment\n",
        "        pass"
      ],
      "metadata": {
        "id": "XIQW5hWk4toi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import itertools\n",
        "import shutil\n",
        "import threading\n",
        "import multiprocessing\n",
        "import time\n",
        "from inspect import getsourcefile\n",
        "from gym.wrappers import Monitor\n",
        "import gym\n",
        "from lib.atari.state_processor import StateProcessor\n",
        "from lib.atari import helpers as atari_helpers\n",
        "from estimators import ValueEstimator, PolicyEstimator\n",
        "from worker import make_copy_params_op, Worker\n",
        "\n",
        "\n",
        "tf.flags.DEFINE_string(\"model_dir\", \"/tmp/a3c\", \"Directory to write Tensorboard summaries and videos to.\")\n",
        "tf.flags.DEFINE_string(\"env\", \"Breakout-v0\", \"Name of gym Atari environment, e.g. Breakout-v0\")\n",
        "tf.flags.DEFINE_integer(\"t_max\", 5, \"Number of steps before performing an update\")\n",
        "tf.flags.DEFINE_integer(\"max_global_steps\", None, \"Stop training after this many steps in the environment. Defaults to running indefinitely.\")\n",
        "tf.flags.DEFINE_integer(\"eval_every\", 300, \"Evaluate the policy every N seconds\")\n",
        "tf.flags.DEFINE_boolean(\"reset\", False, \"If set, delete the existing model directory and start training from scratch.\")\n",
        "tf.flags.DEFINE_integer(\"parallelism\", None, \"Number of threads to run. If not set we run [num_cpu_cores] threads.\")\n",
        "\n",
        "FLAGS = tf.flags.FLAGS\n",
        "\n",
        "current_path = os.path.dirname(os.path.abspath(getsourcefile(lambda:0)))\n",
        "import_path = os.path.abspath(os.path.join(current_path, \"../..\"))\n",
        "if import_path not in sys.path:\n",
        "  sys.path.append(import_path)\n",
        "\n",
        "def make_env(wrap=True):\n",
        "  env = gym.envs.make(FLAGS.env)\n",
        "  env = env.env\n",
        "  if wrap:\n",
        "    env = atari_helpers.AtariEnvWrapper(env)\n",
        "  return env\n",
        "\n",
        "env_ = make_env()\n",
        "if FLAGS.env == \"Pong-v0\" or FLAGS.env == \"Breakout-v0\":\n",
        "  VALID_ACTIONS = list(range(4))\n",
        "else:\n",
        "  VALID_ACTIONS = list(range(env_.action_space.n))\n",
        "env_.close()\n",
        "\n",
        "NUM_WORKERS = multiprocessing.cpu_count()\n",
        "if FLAGS.parallelism:\n",
        "  NUM_WORKERS = FLAGS.parallelism\n",
        "\n",
        "MODEL_DIR = FLAGS.model_dir\n",
        "CHECKPOINT_DIR = os.path.join(MODEL_DIR, \"checkpoints\")\n",
        "\n",
        "if FLAGS.reset:\n",
        "  shutil.rmtree(MODEL_DIR, ignore_errors=True)\n",
        "\n",
        "if not os.path.exists(CHECKPOINT_DIR):\n",
        "  os.makedirs(CHECKPOINT_DIR)\n",
        "\n",
        "summary_writer = tf.summary.FileWriter(os.path.join(MODEL_DIR, \"train\"))\n",
        "\n",
        "with tf.device(\"/cpu:0\"):\n",
        "  global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "  with tf.variable_scope(\"global\") as vs:\n",
        "    policy_net = PolicyEstimator(num_outputs=len(VALID_ACTIONS))\n",
        "    value_net = ValueEstimator(reuse=True)\n",
        "\n",
        "  global_counter = itertools.count()\n",
        "\n",
        "  workers = []\n",
        "  for worker_id in range(NUM_WORKERS):\n",
        "    worker_summary_writer = None\n",
        "    if worker_id == 0:\n",
        "      worker_summary_writer = summary_writer\n",
        "\n",
        "    worker = Worker(\n",
        "      name=\"worker_{}\".format(worker_id),\n",
        "      env=make_env(),\n",
        "      policy_net=policy_net,\n",
        "      value_net=value_net,\n",
        "      global_counter=global_counter,\n",
        "      discount_factor = 0.99,\n",
        "      summary_writer=worker_summary_writer,\n",
        "      max_global_steps=FLAGS.max_global_steps)\n",
        "    workers.append(worker)\n",
        "\n",
        "  saver = tf.train.Saver(keep_checkpoint_every_n_hours=2.0, max_to_keep=10)\n",
        "\n",
        "  pe = PolicyMonitor(\n",
        "    env=make_env(wrap=False),\n",
        "    policy_net=policy_net,\n",
        "    summary_writer=summary_writer,\n",
        "    saver=saver)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  coord = tf.train.Coordinator()\n",
        "\n",
        "  latest_checkpoint = tf.train.latest_checkpoint(CHECKPOINT_DIR)\n",
        "  if latest_checkpoint:\n",
        "    print(\"Loading model checkpoint: {}\".format(latest_checkpoint))\n",
        "    saver.restore(sess, latest_checkpoint)\n",
        "\n",
        "  worker_threads = []\n",
        "  for worker in workers:\n",
        "    worker_fn = lambda worker=worker: worker.run(sess, coord, FLAGS.t_max)\n",
        "    t = threading.Thread(target=worker_fn)\n",
        "    t.start()\n",
        "    worker_threads.append(t)\n",
        "\n",
        "  monitor_thread = threading.Thread(target=lambda: pe.continuous_eval(FLAGS.eval_every, sess, coord))\n",
        "  monitor_thread.start()\n",
        "\n",
        "  coord.join(worker_threads)\n",
        "def build_shared_network(X, add_summaries=False):\n",
        "  conv1 = tf.contrib.layers.conv2d(X, 16, 8, 4, activation_fn=tf.nn.relu, scope=\"conv1\")\n",
        "  conv2 = tf.contrib.layers.conv2d(conv1, 32, 4, 2, activation_fn=tf.nn.relu, scope=\"conv2\")\n",
        "  fc1 = tf.contrib.layers.fully_connected(inputs=tf.contrib.layers.flatten(conv2), num_outputs=256, scope=\"fc1\")\n",
        "  if add_summaries:\n",
        "    tf.contrib.layers.summarize_activation(conv1)\n",
        "    tf.contrib.layers.summarize_activation(conv2)\n",
        "    tf.contrib.layers.summarize_activation(fc1)\n",
        "  return fc1\n",
        "\n",
        "class PolicyEstimator():\n",
        "  def __init__(self, num_outputs, reuse=False, trainable=True):\n",
        "    self.num_outputs = num_outputs\n",
        "    self.states = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
        "    self.targets = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
        "    self.actions = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
        "    X = tf.to_float(self.states) / 255.0\n",
        "    batch_size = tf.shape(self.states)[0]\n",
        "    with tf.variable_scope(\"shared\", reuse=reuse):\n",
        "      fc1 = build_shared_network(X, add_summaries=(not reuse))\n",
        "    with tf.variable_scope(\"policy_net\"):\n",
        "      self.logits = tf.contrib.layers.fully_connected(fc1, num_outputs, activation_fn=None)\n",
        "      self.probs = tf.nn.softmax(self.logits) + 1e-8\n",
        "      self.predictions = {\"logits\": self.logits, \"probs\": self.probs}\n",
        "      self.entropy = -tf.reduce_sum(self.probs * tf.log(self.probs), 1, name=\"entropy\")\n",
        "      self.entropy_mean = tf.reduce_mean(self.entropy, name=\"entropy_mean\")\n",
        "      gather_indices = tf.range(batch_size) * tf.shape(self.probs)[1] + self.actions\n",
        "      self.picked_action_probs = tf.gather(tf.reshape(self.probs, [-1]), gather_indices)\n",
        "      self.losses = - (tf.log(self.picked_action_probs) * self.targets + 0.01 * self.entropy)\n",
        "      self.loss = tf.reduce_sum(self.losses, name=\"loss\")\n",
        "      tf.summary.scalar(self.loss.op.name, self.loss)\n",
        "      tf.summary.scalar(self.entropy_mean.op.name, self.entropy_mean)\n",
        "      tf.summary.histogram(self.entropy.op.name, self.entropy)\n",
        "      if trainable:\n",
        "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
        "        self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
        "        self.grads_and_vars = [[grad, var] for grad, var in self.grads_and_vars if grad is not None]\n",
        "        self.train_op = self.optimizer.apply_gradients(self.grads_and_vars, global_step=tf.contrib.framework.get_global_step())\n",
        "\n",
        "class ValueEstimator():\n",
        "  def __init__(self, reuse=False, trainable=True):\n",
        "    self.states = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
        "    self.targets = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
        "    X = tf.to_float(self.states) / 255.0\n",
        "    with tf.variable_scope(\"shared\", reuse=reuse):\n",
        "      fc1  = build_shared_network(X, add_summaries=(not reuse))\n",
        "\n",
        "    with tf.variable_scope(\"value_net\"):\n",
        "      self.logits = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=1, activation_fn=None)\n",
        "      self.logits = tf.squeeze(self.logits, squeeze_dims=[1], name=\"logits\")\n",
        "      self.losses = tf.squared_difference(self.logits, self.targets)\n",
        "      self.loss = tf.reduce_sum(self.losses, name=\"loss\")\n",
        "      self.predictions = {\"logits\": self.logits}\n",
        "      prefix = tf.get_variable_scope().name\n",
        "      tf.summary.scalar(self.loss.name, self.loss)\n",
        "      tf.summary.scalar(\"{}/max_value\".format(prefix), tf.reduce_max(self.logits))\n",
        "      tf.summary.scalar(\"{}/min_value\".format(prefix), tf.reduce_min(self.logits))\n",
        "      tf.summary.scalar(\"{}/mean_value\".format(prefix), tf.reduce_mean(self.logits))\n",
        "      tf.summary.scalar(\"{}/reward_max\".format(prefix), tf.reduce_max(self.targets))\n",
        "      tf.summary.scalar(\"{}/reward_min\".format(prefix), tf.reduce_min(self.targets))\n",
        "      tf.summary.scalar(\"{}/reward_mean\".format(prefix), tf.reduce_mean(self.targets))\n",
        "      tf.summary.histogram(\"{}/reward_targets\".format(prefix), self.targets)\n",
        "      tf.summary.histogram(\"{}/values\".format(prefix), self.logits)\n",
        "      if trainable:\n",
        "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
        "        self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
        "        self.grads_and_vars = [[grad, var] for grad, var in self.grads_and_vars if grad is not None]\n",
        "        self.train_op = self.optimizer.apply_gradients(self.grads_and_vars, global_step=tf.contrib.framework.get_global_step())\n",
        "\n",
        "    var_scope_name = tf.get_variable_scope().name\n",
        "    summary_ops = tf.get_collection(tf.GraphKeys.SUMMARIES)\n",
        "    sumaries = [s for s in summary_ops if \"policy_net\" in s.name or \"shared\" in s.name]\n",
        "    sumaries = [s for s in summary_ops if var_scope_name in s.name]\n",
        "    self.summaries = tf.summary.merge(sumaries)"
      ],
      "metadata": {
        "id": "1dFmjlTM4xvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyMonitor(object):\n",
        "    def __init__(self, env, policy_net, summary_writer, saver=None):\n",
        "        self.video_dir = os.path.join(summary_writer.get_logdir(), \"../videos\")\n",
        "        self.video_dir = os.path.abspath(self.video_dir)\n",
        "        self.env = Monitor(env, directory=self.video_dir, video_callable=lambda x: True, resume=True)\n",
        "        self.global_policy_net = policy_net\n",
        "        self.summary_writer = summary_writer\n",
        "        self.saver = saver\n",
        "        self.sp = StateProcessor()\n",
        "        self.checkpoint_path = os.path.abspath(os.path.join(summary_writer.get_logdir(), \"../checkpoints/model\"))\n",
        "\n",
        "        try:\n",
        "            os.makedirs(self.video_dir)\n",
        "        except FileExistsError:\n",
        "            pass\n",
        "\n",
        "        with tf.variable_scope(\"policy_eval\"):\n",
        "            self.policy_net = PolicyEstimator(policy_net.num_outputs)\n",
        "\n",
        "        self.copy_params_op = make_copy_params_op(\n",
        "            tf.contrib.slim.get_variables(scope=\"global\", collection=tf.GraphKeys.TRAINABLE_VARIABLES),\n",
        "            tf.contrib.slim.get_variables(scope=\"policy_eval\", collection=tf.GraphKeys.TRAINABLE_VARIABLES))\n",
        "\n",
        "    def _policy_net_predict(self, state, sess):\n",
        "        feed_dict = { self.policy_net.states: [state] }\n",
        "        preds = sess.run(self.policy_net.predictions, feed_dict)\n",
        "        return preds[\"probs\"][0]\n",
        "\n",
        "    def eval_once(self, sess):\n",
        "        with sess.as_default(), sess.graph.as_default():\n",
        "            global_step, _ = sess.run([tf.contrib.framework.get_global_step(), self.copy_params_op])\n",
        "            done = False\n",
        "            state = atari_helpers.atari_make_initial_state(self.sp.process(self.env.reset()))\n",
        "            total_reward = 0.0\n",
        "            episode_length = 0\n",
        "            while not done:\n",
        "                action_probs = self._policy_net_predict(state, sess)\n",
        "                action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                next_state = atari_helpers.atari_make_next_state(state, self.sp.process(next_state))\n",
        "                total_reward += reward\n",
        "                episode_length += 1\n",
        "                state = next_state\n",
        "\n",
        "            episode_summary = tf.Summary()\n",
        "            episode_summary.value.add(simple_value=total_reward, tag=\"eval/total_reward\")\n",
        "            episode_summary.value.add(simple_value=episode_length, tag=\"eval/episode_length\")\n",
        "            self.summary_writer.add_summary(episode_summary, global_step)\n",
        "            self.summary_writer.flush()\n",
        "\n",
        "            if self.saver is not None:\n",
        "                self.saver.save(sess, self.checkpoint_path)\n",
        "\n",
        "            tf.logging.info(\"Eval results at step {}: total_reward {}, episode_length {}\".format(global_step, total_reward, episode_length))\n",
        "\n",
        "            return total_reward, episode_length\n",
        "\n",
        "    def continuous_eval(self, eval_every, sess, coord):\n",
        "        try:\n",
        "            while not coord.should_stop():\n",
        "                self.eval_once(sess)\n",
        "                time.sleep(eval_every)\n",
        "        except tf.errors.CancelledError:\n",
        "            return"
      ],
      "metadata": {
        "id": "nP0zdwT05CNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import dtqn.networks.drqn as drqn\n",
        "from utils import torch_utils\n",
        "import numpy as np\n",
        "from enum import Enum\n",
        "\n",
        "class SoftAttention(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super().__init__()\n",
        "        self.W = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.linear = nn.Linear(embed_size, embed_size)\n",
        "        self.linear2 = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        y = self.W(h.transpose(1, 0))\n",
        "        x = self.linear(x)\n",
        "        z = x + y\n",
        "        z = torch.tanh(z)\n",
        "        z = self.linear2(z)\n",
        "        return F.softmax(z, dim=2)\n",
        "\n",
        "class DARQN(drqn.DRQN):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_shape: int,\n",
        "        n_actions: int,\n",
        "        embed_per_obs_dim: int,\n",
        "        inner_embed: int,\n",
        "        is_discrete_env: bool,\n",
        "        obs_vocab_size: Optional[int] = None,\n",
        "        batch_size: Optional[int] = None,\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        super().__init__(\n",
        "            input_shape=input_shape,\n",
        "            num_actions=n_actions,\n",
        "            embed_per_obs_dim=embed_per_obs_dim,\n",
        "            inner_embed=inner_embed,\n",
        "            is_discrete_env=is_discrete_env,\n",
        "            obs_vocab_size=obs_vocab_size,\n",
        "            **kwargs,\n",
        "        )\n",
        "        self.hidden_zeros = nn.Parameter(\n",
        "            torch.zeros(1, batch_size, inner_embed, dtype=torch.float32),\n",
        "            requires_grad=False,\n",
        "        )\n",
        "        self.attention = SoftAttention(embed_size=inner_embed)\n",
        "        self.apply(torch_utils.init_weights)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.tensor,\n",
        "        hidden_states: Optional[tuple] = None,\n",
        "        episode_lengths: Optional[int] = None,\n",
        "    ):\n",
        "        x = self.obs_embed(x)\n",
        "        if hidden_states is not None:\n",
        "            attention = self.attention(x, hidden_states[0])\n",
        "            lstm_out, hidden_states = self.lstm(attention, hidden_states)\n",
        "            q_values = self.ffn(lstm_out)\n",
        "        else:\n",
        "            q_values = []\n",
        "            hidden_states = (\n",
        "                torch.zeros_like(self.hidden_zeros),\n",
        "                torch.zeros_like(self.hidden_zeros),\n",
        "            )\n",
        "            context_len = x.size(1)\n",
        "            for i in range(context_len):\n",
        "                attention = self.attention(x[:, i : i + 1, :], hidden_states[0])\n",
        "                lstm_out, hidden_states = self.lstm(attention, hidden_states)\n",
        "                q = self.ffn(lstm_out)\n",
        "                q_values.append(q)\n",
        "            q_values = torch.cat(q_values, dim=1)\n",
        "\n",
        "        return q_values, hidden_states\n",
        "\n",
        "class GRUGate(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        embed_size = kwargs[\"embed_size\"]\n",
        "        self.w_r = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.u_r = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.w_z = nn.Linear(embed_size, embed_size)\n",
        "        self.u_z = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.w_g = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.u_g = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.init_bias()\n",
        "\n",
        "    def init_bias(self):\n",
        "        with torch.no_grad():\n",
        "            self.w_z.bias.fill_(-2)  # This is the value set by GTrXL paper\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        z = torch.sigmoid(self.w_z(y) + self.u_z(x))\n",
        "        r = torch.sigmoid(self.w_r(y) + self.u_r(x))\n",
        "        h = torch.tanh(self.w_g(y) + self.u_g(r * x))\n",
        "        return (1.0 - z) * x + z * h\n",
        "\n",
        "class ResGate(nn.Module):\n",
        "    \"\"\"Residual skip connection\"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        return x + y\n",
        "\n",
        "class PosEnum(Enum):\n",
        "    LEARNED = \"learned\"\n",
        "    SIN = \"sin\"\n",
        "    NONE = \"none\"\n",
        "\n",
        "class PositionEncoding(nn.Module):\n",
        "    def __init__(self, position_encoding: nn.Module):\n",
        "        super().__init__()\n",
        "        self.position_encoding = position_encoding\n",
        "\n",
        "    def forward(self):\n",
        "        return self.position_encoding\n",
        "\n",
        "    @staticmethod\n",
        "    def make_sinusoidal_position_encoding(\n",
        "        context_len: int, embed_dim: int\n",
        "    ) -> PositionEncoding:\n",
        "        position = torch.arange(context_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, embed_dim, 2) * (-np.log(10000.0) / embed_dim)\n",
        "        )\n",
        "        pos_encoding = torch.zeros(1, context_len, embed_dim)\n",
        "        pos_encoding[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pos_encoding[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        return PositionEncoding(nn.Parameter(pos_encoding, requires_grad=False))\n",
        "\n",
        "    @staticmethod\n",
        "    def make_learned_position_encoding(\n",
        "        context_len: int, embed_dim: int\n",
        "    ) -> PositionEncoding:\n",
        "        return PositionEncoding(\n",
        "            nn.Parameter(torch.zeros(1, context_len, embed_dim), requires_grad=True)\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def make_empty_position_encoding(\n",
        "        context_len: int, embed_dim: int\n",
        "    ) -> PositionEncoding:\n",
        "        return PositionEncoding(\n",
        "            nn.Parameter(torch.zeros(1, context_len, embed_dim), requires_grad=False)\n",
        "        )\n",
        "\n",
        "class ObservationEmbeddingRepresentation(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_embedding: nn.Module,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.observation_embedding = observation_embedding\n",
        "\n",
        "    def forward(self, obs: torch.Tensor):\n",
        "        batch, seq = obs.size(0), obs.size(1)\n",
        "        obs = torch.flatten(obs, start_dim=0, end_dim=1)\n",
        "        obs_embed = self.observation_embedding(obs)\n",
        "        obs_embed = obs_embed.reshape(batch, seq, obs_embed.size(-1))\n",
        "        return obs_embed\n",
        "\n",
        "    @staticmethod\n",
        "    def make_discrete_representation(\n",
        "        vocab_sizes: int, obs_dim: int, embed_per_obs_dim: int, outer_embed_size: int\n",
        "    ) -> ObservationEmbeddingRepresentation:\n",
        "        assert (\n",
        "            vocab_sizes > 0\n",
        "        ), \"Discrete environments need to have a vocab size for the token embeddings\"\n",
        "        assert (\n",
        "            embed_per_obs_dim > 1\n",
        "        ), \"Each observation feature needs at least 1 embed dim\"\n",
        "\n",
        "        embedding = nn.Sequential(\n",
        "            nn.Embedding(vocab_sizes, embed_per_obs_dim),\n",
        "            nn.Flatten(start_dim=-2),\n",
        "            nn.Linear(embed_per_obs_dim * obs_dim, outer_embed_size),\n",
        "        )\n",
        "        return ObservationEmbeddingRepresentation(observation_embedding=embedding)\n",
        "\n",
        "    @staticmethod\n",
        "    def make_action_representation(\n",
        "        num_actions: int,\n",
        "        action_dim: int,\n",
        "    ) -> ObservationEmbeddingRepresentation:\n",
        "        embed = nn.Sequential(\n",
        "            nn.Embedding(num_actions, action_dim), nn.Flatten(start_dim=-2)\n",
        "        )\n",
        "        return ObservationEmbeddingRepresentation(observation_embedding=embed)\n",
        "\n",
        "    @staticmethod\n",
        "    def make_continuous_representation(obs_dim: int, outer_embed_size: int):\n",
        "        embedding = nn.Linear(obs_dim, outer_embed_size)\n",
        "        return ObservationEmbeddingRepresentation(observation_embedding=embedding)\n",
        "\n",
        "    @staticmethod\n",
        "    def make_image_representation(obs_dim: Tuple, outer_embed_size: int):\n",
        "        if len(obs_dim) == 3:\n",
        "            num_channels = obs_dim[0]\n",
        "        else:\n",
        "            num_channels = 1\n",
        "\n",
        "        kernels = [3, 3, 3, 3, 3]\n",
        "        paddings = [1, 1, 1, 1, 1]\n",
        "        strides = [2, 1, 2, 1, 2]\n",
        "        flattened_size = compute_flattened_size(\n",
        "            obs_dim[1], obs_dim[2], kernels, paddings, strides\n",
        "        )\n",
        "        embedding = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                num_channels,\n",
        "                64,\n",
        "                kernel_size=kernels[0],\n",
        "                padding=paddings[0],\n",
        "                stride=strides[0],\n",
        "            ),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(\n",
        "                64, 64, kernel_size=kernels[1], padding=paddings[1], stride=strides[1]\n",
        "            ),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(\n",
        "                64,\n",
        "                64,\n",
        "                kernel_size=kernels[2],\n",
        "                padding=paddings[2],\n",
        "                stride=strides[2],\n",
        "            ),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(\n",
        "                64, 128, kernel_size=kernels[3], padding=paddings[3], stride=strides[3]\n",
        "            ),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * flattened_size, outer_embed_size),\n",
        "        )\n",
        "        return ObservationEmbeddingRepresentation(observation_embedding=embedding)\n",
        "\n",
        "def compute_flattened_size(\n",
        "    height: int, width: int, kernels: list, paddings: list, strides: list\n",
        ") -> int:\n",
        "    for i in range(len(kernels)):\n",
        "        height = update_size(height, kernels[i], paddings[i], strides[i])\n",
        "        width = update_size(width, kernels[i], paddings[i], strides[i])\n",
        "    return int(height * width)\n",
        "\n",
        "def update_size(component: int, kernel: int, padding: int, stride: int) -> int:\n",
        "    return math.floor((component - kernel + 2 * padding) / stride) + 1\n",
        "\n",
        "class ActionEmbeddingRepresentation(nn.Module):\n",
        "    def __init__(self, num_actions: int, action_dim: int):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Sequential(\n",
        "            nn.Embedding(num_actions, action_dim),\n",
        "            nn.Flatten(start_dim=-2),\n",
        "        )\n",
        "\n",
        "    def forward(self, action: torch.Tensor):\n",
        "        return self.embedding(action)\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "\n",
        "    Args:\n",
        "        num_heads:  Number of heads to use for MultiHeadAttention.\n",
        "        embed_size: The dimensionality of the layer.\n",
        "        history_len:The maximum number of observations to take in.\n",
        "        dropout:    Dropout percentage.\n",
        "        attn_gate:  The combine layer after the attention submodule.\n",
        "        mlp_gate:  The combine layer after the feedforward submodule.\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_heads: int,\n",
        "        embed_size: int,\n",
        "        history_len: int,\n",
        "        dropout: float,\n",
        "        attn_gate,\n",
        "        mlp_gate,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layernorm1 = nn.LayerNorm(embed_size)\n",
        "        self.layernorm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=embed_size,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_size, 4 * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * embed_size, embed_size),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.attn_gate = attn_gate\n",
        "        self.mlp_gate = mlp_gate\n",
        "        self.alpha = None\n",
        "        self.attn_mask = nn.Parameter(\n",
        "            torch.triu(torch.ones(history_len, history_len), diagonal=1),\n",
        "            requires_grad=False,\n",
        "        )\n",
        "        self.attn_mask[self.attn_mask.bool()] = -float(\"inf\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        attention, self.alpha = self.attention(\n",
        "            x,\n",
        "            x,\n",
        "            x,\n",
        "            attn_mask=self.attn_mask[: x.size(1), : x.size(1)],\n",
        "            average_attn_weights=True,  # Only affects self.alpha for visualizations\n",
        "        )\n",
        "        x = self.attn_gate(x, F.relu(attention))\n",
        "        x = self.layernorm1(x)\n",
        "        ffn = self.ffn(x)\n",
        "        x = self.mlp_gate(x, F.relu(ffn))\n",
        "        x = self.layernorm2(x)\n",
        "        return x\n",
        "\n",
        "class TransformerIdentityLayer(TransformerLayer):\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x_norm1 = self.layernorm1(x)\n",
        "        attention, self.alpha = self.attention(\n",
        "            x_norm1,\n",
        "            x_norm1,\n",
        "            x_norm1,\n",
        "            attn_mask=self.attn_mask[: x_norm1.size(1), : x_norm1.size(1)],\n",
        "            average_attn_weights=True,  # Only affects self.alpha for visualizations\n",
        "        )\n",
        "        x = self.attn_gate(x, F.relu(attention))\n",
        "        x_norm2 = self.layernorm2(x)\n",
        "        ffn = self.ffn(x_norm2)\n",
        "        x = self.mlp_gate(x, F.relu(ffn))\n",
        "        return x"
      ],
      "metadata": {
        "id": "8apP4WTH5nm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Aggregator(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        output_dim: int,\n",
        "        aggregator_type: str = \"flatten\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.aggregator_type = aggregator_type\n",
        "\n",
        "        if self.aggregator_type == \"flatten\":\n",
        "            self.aggregator = nn.Flatten(start_dim=1)\n",
        "        elif self.aggregator_type == \"mean\":\n",
        "            self.aggregator = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.aggregator_type == \"flatten\":\n",
        "            return self.aggregator(x)\n",
        "        elif self.aggregator_type == \"mean\":\n",
        "            return torch.mean(x, dim=1)\n",
        "\n",
        "class ConcatenationModule(nn.Module):\n",
        "    \"\"\"Concatenates multiple input tensors along the specified dimension.\"\"\"\n",
        "\n",
        "    def __init__(self, dim: int = 1):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, *inputs: torch.Tensor) -> torch.Tensor:\n",
        "        return torch.cat(inputs, dim=self.dim)\n",
        "\n",
        "class QValueHead(nn.Module):\n",
        "    \"\"\"Module that generates Q-values from the given input.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int, num_actions: int):\n",
        "        super().__init__()\n",
        "        self.output_dim = num_actions\n",
        "        self.linear = nn.Linear(input_dim, self.output_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.linear(x)\n",
        "\n",
        "class SoftmaxHead(nn.Module):\n",
        "    \"\"\"Module that applies softmax activation to the input.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return F.softmax(x, dim=-1)\n",
        "\n",
        "class DARQN(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_representation: nn.Module,\n",
        "        action_representation: nn.Module,\n",
        "        aggregator: nn.Module,\n",
        "        transformer: nn.Module,\n",
        "        q_head: nn.Module,\n",
        "        softmax_head: nn.Module,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.observation_representation = observation_representation\n",
        "        self.action_representation = action_representation\n",
        "        self.aggregator = aggregator\n",
        "        self.transformer = transformer\n",
        "        self.q_head = q_head\n",
        "        self.softmax_head = softmax_head\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        observations: torch.Tensor,\n",
        "        actions: torch.Tensor,\n",
        "        hidden_states: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        obs_embed = self.observation_representation(observations)\n",
        "        action_embed = self.action_representation(actions)\n",
        "        aggregated_obs_embed = self.aggregator(obs_embed)\n",
        "        concatenated_input = ConcatenationModule(dim=1)(\n",
        "            aggregated_obs_embed, action_embed\n",
        "        )\n",
        "        transformer_output = self.transformer(concatenated_input)\n",
        "        q_values = self.q_head(transformer_output)\n",
        "        return self.softmax_head(q_values)\n",
        "observation_representation = ObservationEmbeddingRepresentation.make_image_representation(\n",
        "    obs_dim=(3, 84, 84), outer_embed_size=512\n",
        ")\n",
        "action_representation = ActionEmbeddingRepresentation.make_action_representation(\n",
        "    num_actions=10, action_dim=32\n",
        ")\n",
        "aggregator = Aggregator(input_dim=512 + 32, output_dim=512, aggregator_type=\"mean\")\n",
        "transformer = TransformerIdentityLayer(\n",
        "    num_heads=8, embed_size=512, history_len=4, dropout=0.1, attn_gate=ResGate(), mlp_gate=ResGate()\n",
        ")\n",
        "q_head = QValueHead(input_dim=512, num_actions=10)\n",
        "softmax_head = SoftmaxHead(input_dim=10)\n",
        "\n",
        "darqn = DARQN(\n",
        "    observation_representation=observation_representation,\n",
        "    action_representation=action_representation,\n",
        "    aggregator=aggregator,\n",
        "    transformer=transformer,\n",
        "    q_head=q_head,\n",
        "    softmax_head=softmax_head,\n",
        ")"
      ],
      "metadata": {
        "id": "Fig8RKIZ7G-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "from collections import namedtuple\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "from ding.torch_utils import to_device\n",
        "from ding.utils import POLICY_REGISTRY\n",
        "from ding.utils.data import default_decollate\n",
        "from .base_policy import Policy\n",
        "\n",
        "\n",
        "@POLICY_REGISTRY.register('dt')\n",
        "class DTPolicy(Policy):\n",
        "\n",
        "    config = dict(\n",
        "        # (str) RL policy register name (refer to function \"POLICY_REGISTRY\").\n",
        "        type='dt',\n",
        "        # (bool) Whether to use cuda for network.\n",
        "        cudaa=False,  # introduced a spelling mistake\n",
        "        # (bool) Whether the RL algorithm is on-policy or off-policy.\n",
        "        on_policy=False,\n",
        "        # (bool) Whether use priority(priority sample, IS weight, update priority)\n",
        "        priority=False,\n",
        "        # (int) N-step reward for target q_value estimation\n",
        "        obz_shape=4,  # introduced a spelling mistake\n",
        "        action_shapee=2,  # introduced a spelling mistake\n",
        "        rtg_scale=1000,  # normalize returns to go\n",
        "        max_eval_ep_len=1000,  # max len of one episode\n",
        "        batch_size=64,  # training batch size\n",
        "        wt_decay=1e-4,  # decay weight in optimizer\n",
        "        warmup_steps=10000,  # steps for learning rate warmup\n",
        "        context_len=20,  # length of transformer input\n",
        "        learning_rate=1e-4,\n",
        "    )\n",
        "\n",
        "    def default_model_settings(self) -> Tuple[str, List[str]]:\n",
        "\n",
        "        return 'dt', ['ding.model.template.dt']\n",
        "\n",
        "    def init_learning(self) -> None:\n",
        "\n",
        "\n",
        "        self.rtg_scalee = self._cfg.rtg_scale  # introduced a spelling mistake  # normalize returns to go\n",
        "        self.rtg_target = self._cfg.rtg_target  # max target reward_to_go\n",
        "        self.max_eval_ep_len = self._cfg.max_eval_ep_len  # max len of one episode\n",
        "\n",
        "        lr = self._cfg.learning_rate  # learning rate\n",
        "        wt_decay = self._cfg.wt_decay  # weight decay\n",
        "        warmup_steps = self._cfg.warmup_steps  # warmup steps for lr scheduler\n",
        "\n",
        "        self.clip_grad_norm_p = self._cfg.clip_grad_norm_p\n",
        "        self.context_len = self._cfg.model.context_len  # K in decision transformer\n",
        "\n",
        "        self.state_dim = self._cfg.model.state_dimm  # introduced a spelling mistake\n",
        "        self.act_dim = self._cfg.model.act_dimm  # introduced a spelling mistake\n",
        "\n",
        "        self._learn_model = self._model\n",
        "        self._atari_env = 'state_mean' not in self._cfg\n",
        "        self._basic_discrete_env = not self._cfg.model.continuous and 'state_mean' in self._cfg\n",
        "\n",
        "        if self._atari_env:\n",
        "            self._optimizer = self._learn_model.configure_optimizers(wt_decay, lr)\n",
        "        else:\n",
        "            self._optimizer = torch.optim.AdamW(self._learn_model.parameters(), lr=lr, weight_decay=wt_decay)\n",
        "\n",
        "        self._scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
        "            self._optimizer, lambda steps: min((steps + 1) / warmup_steps, 1)\n",
        "        )\n",
        "\n",
        "        self.max_env_scoree = -1.0  # introduced a spelling mistake\n",
        "\n",
        "    def forward_learning(self, data: List[torch.Tensor]) -> Dict[str, Any]:\n",
        "\n",
        "\n",
        "        self._learn_model.train()\n",
        "\n",
        "        timesteps, states, actions, returns_to_go, traj_mask = data\n",
        "\n",
        "         # and we need a 3-dim tensor\n",
        "        if len(returns_to_go.shape) == 2:\n",
        "            returns_to_go = returns_to_go.unsqueeze(-1)\n",
        "\n",
        "        if self._basic_discrete_env:\n",
        "            actions = actions.to(torch.long)\n",
        "            actions = actions.squeeze(-1)\n",
        "        action_target = torch.clone(actions).detach().to(self._device)\n",
        "\n",
        "        if self._atari_env:\n",
        "            state_preds, action_preds, return_preds = self._learn_model.forward(\n",
        "                timesteps=timesteps, states=states, actions=actions, returns_to_go=returns_to_go, tar=1\n",
        "            )\n",
        "        else:\n",
        "            state_preds, action_preds, return_preds = self._learn_model.forward(\n",
        "                timesteps=timesteps, states=states, actions=actions, returns_to_go=returns_to_go\n",
        "            )\n",
        "\n",
        "        if self._atari_env:\n",
        "            action_loss = F.cross_entropy(action_preds.reshape(-1, action_preds.size(-1)), action_target.reshape(-1))\n",
        "        else:\n",
        "            traj_mask = traj_mask.view(-1, )\n",
        "\n",
        "            # only consider non padded elements\n",
        "            action_preds = action_preds.view(-1, self.act_dim)[traj_mask > 0]\n",
        "\n",
        "            if self._cfg.model.continuous:\n",
        "                action_target = action_target.view(-1, self.act_dim)[traj_mask > 0]\n",
        "                action_loss = F.mse_loss(action_preds, action_target)\n",
        "            else:\n",
        "                action_target = action_target.view(-1)[traj_mask > 0]\n",
        "                action_loss = F.cross_entropy(action_preds, action_target)\n",
        "\n",
        "        self._optimizer.zero_grad()\n",
        "        action_loss.backward()\n",
        "        if self._cfg.multi_gpu:\n",
        "            self.sync_gradients(self._learn_model)\n",
        "        torch.nn.utils.clip_grad_norm_(self._learn_model.parameters(), self.clip_grad_norm_p)\n",
        "        self._optimizer.step()\n",
        "        self._scheduler.step()\n",
        "\n",
        "        return {\n",
        "            'cur_lr': self._optimizer.state_dict()['param_groups'][0]['lr'],\n",
        "            'action_loss': action_loss.detach().cpu().item(),\n",
        "            'total_loss': action_loss.detach().cpu().item(),\n",
        "        }\n",
        "\n",
        "    def init_evaluation(self) -> None:\n",
        "\n",
        "        self._eval_model = self._model\n",
        "        # init data\n",
        "        self._device = torch.device(self._device)\n",
        "        self.rtg_scale = self._cfg.rtg_scale  # normalize returns to go\n",
        "        self.rtg_target = self._cfg.rtg_target  # max target reward_to_go\n",
        "        self.state_dim = self._cfg.model.state_dim\n",
        "        self.act_dim = self._cfg.model.act_dim\n",
        "        self.eval_batch_size = self._cfg.evaluator_env_num\n",
        "        self.max_eval_ep_len = self._cfg.max_eval_ep_len\n",
        "        self.context_len = self._cfg.model.context_len  # K in decision transformer\n",
        "\n",
        "        self.t = [0 for _ in range(self.eval_batch_size)]\n",
        "        if self._cfg.model.continuous:\n",
        "            self.actions = torch.zeros(\n",
        "                (self.eval_batch_size, self.max_eval_ep_len, self.act_dim), dtype=torch.float32, device=self._device\n",
        "            )\n",
        "        else:\n",
        "            self.actions = torch.zeros(\n",
        "                (self.eval_batch_size, self.max_eval_ep_len, 1), dtype=torch.long, device=self._device\n",
        "            )\n",
        "        self._atari_env = 'state_mean' not in self._cfg\n",
        "        self._basic_discrete_env = not self._cfg.model.continuous and 'state_mean' in self._cfg\n",
        "        if self._atari_env:\n",
        "            self.states = torch.zeros(\n",
        "                (\n",
        "                    self.eval_batch_size,\n",
        "                    self.max_eval_ep_len,\n",
        "                ) + tuple(self.state_dim),\n",
        "                dtype=torch.float32,\n",
        "                device=self._device\n",
        "            )\n",
        "            self.running_rtg = [self.rtg_target for _ in range(self.eval_batch_size)]\n",
        "        else:\n",
        "            self.running_rtg = [self.rtg_target / self.rtg_scale for _ in range(self.eval_batch_size)]\n",
        "            self.states = torch.zeros(\n",
        "                (self.eval_batch_size, self.max_eval_ep_len, self.state_dim), dtype=torch.float32, device=self._device\n",
        "            )\n",
        "            self.state_mean = torch.from_numpy(np.array(self._cfg.state_mean)).to(self._device)\n",
        "            self.state_std = torch.from_numpy(np.array(self._cfg.state_std)).to(self._device)\n",
        "        self.timesteps = torch.arange(\n",
        "            start=0, end=self.max_eval_ep_len, step=1\n",
        "        ).repeat(self.eval_batch_size, 1).to(self._device)\n",
        "        self.rewards_to_go = torch.zeros(\n",
        "            (self.eval_batch_size, self.max_eval_ep_len, 1), dtype=torch.float32, device=self._device\n",
        "        )\n",
        "\n",
        "    def forward_evaluation(self, data: Dict[int, Any]) -> Dict[int, Any]:\n",
        "\n",
        "        # save and forward\n",
        "        data_id = list(data.keys())\n",
        "\n",
        "        self._eval_model.eval()\n",
        "        with torch.no_grad():\n",
        "            if self._atari_env:\n",
        "                states = torch.zeros(\n",
        "                    (\n",
        "                        self.eval_batch_size,\n",
        "                        self.context_len,\n",
        "                    ) + tuple(self.state_dim),\n",
        "                    dtype=torch.float32,\n",
        "                    device=self._device\n",
        "                )\n",
        "                timesteps = torch.zeros((self.eval_batch_size, 1, 1), dtype=torch.long, device=self._device)\n",
        "            else:\n",
        "                states = torch.zeros(\n",
        "                    (self.eval_batch_size, self.context_len, self.state_dim), dtype=torch.float32, device=self._device\n",
        "                )\n",
        "                timesteps = torch.zeros((self.eval_batch_size, self.context_len), dtype=torch.long, device=self._device)\n",
        "            if not self._cfg.model.continuous:\n",
        "                actions = torch.zeros(\n",
        "                    (self.eval_batch_size, self.context_len, 1), dtype=torch.long, device=self._device\n",
        "                )\n",
        "            else:\n",
        "                actions = torch.zeros(\n",
        "                    (self.eval_batch_size, self.context_len, self.act_dim), dtype=torch.float32, device=self._device\n",
        "                )\n",
        "            rewards_to_go = torch.zeros(\n",
        "                (self.eval_batch_size, self.context_len, 1), dtype=torch.float32, device=self._device\n",
        "            )\n",
        "            for i in data_id:\n",
        "                if self._atari_env:\n",
        "                    self.states[i, self.t[i]] = data[i]['obs'].to(self._device)\n",
        "                else:\n",
        "                    self.states[i, self.t[i]] = (data[i]['obs'].to(self._device) - self.state_mean) / self.state_std\n",
        "                self.running_rtg[i] = self.running_rtg[i] - (data[i]['reward'] / self.rtg_scale).to(self._device)\n",
        "                self.rewards_to_go[i, self.t[i]] = self.running_rtg[i]\n",
        "\n",
        "                if self.t[i] <= self.context_len:\n",
        "                    if self._atari_env:\n",
        "                        timesteps[i] = min(self.t[i], self._cfg.model.max_timestep) * torch.ones(\n",
        "                            (1, 1), dtype=torch.int64\n",
        "                        ).to(self._device)\n",
        "                    else:\n",
        "                        timesteps[i] = self.timesteps[i, :self.context_len]\n",
        "                    states[i] = self.states[i, :self.context_len]\n",
        "                    actions[i] = self.actions[i, :self.context_len]\n",
        "                    rewards_to_go[i] = self.rewards_to_go[i, :self.context_len]\n",
        "                else:\n",
        "                    if self._atari_env:\n",
        "                        timesteps[i] = min(self.t[i], self._cfg.model.max_timestep) * torch.ones(\n",
        "                            (1, 1), dtype=torch.int64\n",
        "                        ).to(self._device)\n",
        "                    else:\n",
        "                        timesteps[i] = self.timesteps[i, self.t[i] - self.context_len + 1:self.t[i] + 1]\n",
        "                    states[i] = self.states[i, self.t[i] - self.context_len + 1:self.t[i] + 1]\n",
        "                    actions[i] = self.actions[i, self.t[i] - self.context_len + 1:self.t[i] + 1]\n",
        "                    rewards_to_go[i] = self.rewards_to_go[i, self.t[i] - self.context_len + 1:self.t[i] + 1]\n",
        "            if self._basic_discrete_env:\n",
        "                actions = actions.squeeze(-1)\n",
        "            _, act_preds, _ = self._eval_model.forward(timesteps, states, actions, rewards_to_go)\n",
        "            del timesteps, states, actions, rewards_to_go\n",
        "\n",
        "            logits = act_preds[:, -1, :]\n",
        "            if not self._cfg.model.continuous:\n",
        "                if self._atari_env:\n",
        "                    probs = F.softmax(logits, dim=-1)\n",
        "                    act = torch.zeros((self.eval_batch_size, 1), dtype=torch.long, device=self._device)\n",
        "                    for i in data_id:\n",
        "                        act[i] = torch.multinomial(probs[i], num_samples=1)\n",
        "                else:\n",
        "                    act = torch.argmax(logits, axis=1).unsqueeze(1)\n",
        "            else:\n",
        "                act = logits\n",
        "            for i in data_id:\n",
        "                self.actions[i, self.t[i]] = act[i]  # TODO: self.actions[i] should be a queue when exceed max_t\n",
        "                self.t[i] += 1\n",
        "\n",
        "        if self._cuda:\n",
        "            act = to_device(act, 'cpu')\n",
        "        output = {'action': act}\n",
        "        output = default_decollate(output)\n",
        "        return {i: d for i, d in zip(data_id, output)}\n",
        "\n",
        "    def reset_evaluation(self, data_id: Optional[List[int]] = None) -> None:\n",
        "\n",
        "\n",
        "        if data_id is None:\n",
        "            self.t = [0 for _ in range(self.eval_batch_size)]\n",
        "            self.timesteps = torch.arange(\n",
        "                start=0, end=self.max_eval_ep_len, step=1\n",
        "            ).repeat(self.eval_batch_size, 1).to(self._device)\n",
        "            if not self._cfg.model.continuous:\n",
        "                self.actions = torch.zeros(\n",
        "                    (self.eval_batch_size, self.max_eval_ep_len, 1), dtype=torch.long, device=self._device\n",
        "                )\n",
        "            else:\n",
        "                self.actions = torch.zeros(\n",
        "                    (self.eval_batch_size, self.max_eval_ep_len, self.act_dim),\n",
        "                    dtype=torch.float32,\n",
        "                    device=self._device\n",
        "                )\n",
        "            if self._atari_env:\n",
        "                self.states = torch.zeros(\n",
        "                    (\n",
        "                        self.eval_batch_size,\n",
        "                        self.max_eval_ep_len,\n",
        "                    ) + tuple(self.state_dim),\n",
        "                    dtype=torch.float32,\n",
        "                    device=self._device\n",
        "                )\n",
        "                self.running_rtg = [self.rtg_target for _ in range(self.eval_batch_size)]\n",
        "            else:\n",
        "                self.states = torch.zeros(\n",
        "                    (self.eval_batch_size, self.max_eval_ep_len, self.state_dim),\n",
        "                    dtype=torch.float32,\n",
        "                    device=self._device\n",
        "                )\n",
        "                self.running_rtg = [self.rtg_target / self.rtg_scale for _ in range(self.eval_batch_size)]\n",
        "\n",
        "            self.rewards_to_go = torch.zeros(\n",
        "                (self.eval_batch_size, self.max_eval_ep_len, 1), dtype=torch.float32, device=self._device\n",
        "            )\n",
        "        else:\n",
        "            for i in data_id:\n",
        "                self.t[i] = 0\n",
        "                if not self._cfg.model.continuous:\n",
        "                    self.actions[i] = torch.zeros((self.max_eval_ep_len, 1), dtype=torch.long, device=self._device)\n",
        "                else:\n",
        "                    self.actions[i] = torch.zeros(\n",
        "                        (self.max_eval_ep_len, self.act_dim), dtype=torch.float32, device=self._device\n",
        "                    )\n",
        "                if self._atari_env:\n",
        "                    self.states[i] = torch.zeros(\n",
        "                        (self.max_eval_ep_len, ) + tuple(self.state_dim), dtype=torch.float32, device=self._device\n",
        "                    )\n",
        "                    self.running_rtg[i] = self.rtg_target\n",
        "                else:\n",
        "                    self.states[i] = torch.zeros(\n",
        "                        (self.max_eval_ep_len, self.state_dim), dtype=torch.float32, device=self._device\n",
        "                    )\n",
        "                    self.running_rtg[i] = self.rtg_target / self.rtg_scale\n",
        "                    self.timesteps[i] = torch.arange(start=0, end=self.max_eval_ep_len, step=1).to(self._device)\n",
        "                self.rewards_to_go[i] = torch.zeros((self.max_eval_ep_len, 1), dtype=torch.float32, device=self._device)"
      ],
      "metadata": {
        "id": "ADqWrxw08mBB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}