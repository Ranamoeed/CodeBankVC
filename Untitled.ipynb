{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ranamoeed/CodeBankVC/blob/main/Untitled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from functools import lru_cache\n",
        "import threading\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "class WebDataFetcherProcessor:\n",
        "    def __init__(self, user_agents=None, timeout=30):\n",
        "        self.user_agents = user_agents or [\n",
        "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
        "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n",
        "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
        "        ]\n",
        "        self.timeout = timeout\n",
        "        self.image_download_folder = \"images\"\n",
        "\n",
        "    def _make_request(self, url):\n",
        "        try:\n",
        "            headers = self._get_random_user_agent()\n",
        "            response = requests.get(url, headers=headers, timeout=self.timeout)\n",
        "            response.raise_for_status()\n",
        "            return response.text\n",
        "        except requests.RequestException as e:\n",
        "            logging.error(f\"Request failed for URL '{url}': {e}\")\n",
        "            return None\n",
        "\n",
        "    def _get_random_user_agent(self):\n",
        "        return {'User-Agent': random.choice(self.user_agents)}\n",
        "\n",
        "    @lru_cache(maxsize=128)\n",
        "    def fetch_website_content(self, url):\n",
        "        try:\n",
        "            html = self._make_request(url)\n",
        "            if not html:\n",
        "                return None\n",
        "            soup = BeautifulSoup(html, 'html.parser')\n",
        "            text_content = \"\\n\".join([p.text.strip() for p in soup.find_all('p') if p.text.strip()])\n",
        "            meta_data = self.extract_metadata(soup)\n",
        "            image_urls = [img['src'] for img in soup.find_all('img') if img.get('src')]\n",
        "            return {\"text_content\": text_content, \"meta_data\": meta_data, \"image_urls\": image_urls}\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error parsing website content from {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def extract_metadata(self, soup):\n",
        "        title = soup.title.string.strip() if soup.title else \"\"\n",
        "        description = soup.find(\"meta\", {\"name\": \"description\"})['content'].strip() if soup.find(\n",
        "            \"meta\", {\"name\": \"description\"}) else \"\"\n",
        "        return {\"title\": title, \"description\": description}\n",
        "\n",
        "    def download_image(self, url):\n",
        "        try:\n",
        "            headers = self._get_random_user_agent()\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            image_data = response.content\n",
        "            filename = url.split(\"/\")[-1]\n",
        "            image_path = os.path.join(self.image_download_folder, filename)\n",
        "            with open(image_path, \"wb\") as f:\n",
        "                f.write(image_data)\n",
        "            return image_path\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error downloading image from {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    @lru_cache(maxsize=128)\n",
        "    def search(self, query, num_results=10):\n",
        "        try:\n",
        "            search_url = f\"https://duckduckgo.com/html/?q={query.replace(' ', '+')}\"\n",
        "            html = self._make_request(search_url)\n",
        "            if html:\n",
        "                soup = BeautifulSoup(html, 'html.parser')\n",
        "                links = [a['href'] for a in soup.find_all('a', class_='result__a')]\n",
        "                return links[:num_results]\n",
        "            else:\n",
        "                return []\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error searching for '{query}': {e}\")\n",
        "            return []\n",
        "\n",
        "    def process_query(self, query, num_websites=10):\n",
        "        try:\n",
        "            search_results = self.search(query, num_websites)\n",
        "            if not search_results:\n",
        "                logging.warning(\"No search results found.\")\n",
        "                return []\n",
        "            websites_data = []\n",
        "            for link in search_results:\n",
        "                website_data = self.fetch_website_content(link)\n",
        "                if website_data:\n",
        "                    website_data[\"image_paths\"] = [self.download_image(img_url) for img_url in website_data[\"image_urls\"]]\n",
        "                    websites_data.append(website_data)\n",
        "            return websites_data\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error processing query '{query}': {e}\")\n",
        "            return []\n",
        "\n",
        "def process_query_on_thread(query, num_websites):\n",
        "    web_data_fetcher_processor = WebDataFetcherProcessor()\n",
        "    return web_data_fetcher_processor.process_query(query, num_websites)\n",
        "\n",
        "def main(queries):\n",
        "    threads = []\n",
        "    for query in queries:\n",
        "        thread = threading.Thread(target=process_query_on_thread, args=(query,))\n",
        "        threads.append(thread)\n",
        "        thread.start()\n",
        "\n",
        "    for thread in threads:\n",
        "        thread.join()"
      ],
      "metadata": {
        "id": "z3Yd0IFAwcTo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}